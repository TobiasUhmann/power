{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mar 6\n",
    "\n",
    "Visualize how well the class embeddings attend on words and\n",
    "sentences. The expected result would be that the “married”\n",
    "class embedding, for example, attends heavily on words and\n",
    "sentences related to marriage like “married”, “husband”, “wife”, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.display import display\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch import tensor, Tensor\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from classifier import Classifier\n",
    "from dao.ower.ower_dir import Sample, OwerDir\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ower_dataset = 'ower-v3-fb-irt-3'\n",
    "class_count = 4\n",
    "sent_count = 3\n",
    "\n",
    "# vectors = None\n",
    "# vectors = 'charngram.100d'\n",
    "# vectors = 'fasttext.en.300d'\n",
    "# vectors = 'fasttext.simple.300d'\n",
    "# vectors = 'glove.42B.300d'\n",
    "# vectors = 'glove.840B.300d'\n",
    "# vectors = 'glove.twitter.27B.25d'\n",
    "# vectors = 'glove.twitter.27B.50d'\n",
    "# vectors = 'glove.twitter.27B.100d'\n",
    "vectors = 'glove.twitter.27B.200d'\n",
    "# vectors = 'glove.6B.50d'\n",
    "# vectors = 'glove.6B.100d'\n",
    "# vectors = 'glove.6B.200d'\n",
    "# vectors = 'glove.6B.300d'\n",
    "\n",
    "emb_size = None\n",
    "# emb_size = 200\n",
    "\n",
    "batch_size = 1024\n",
    "sent_len = 64\n",
    "\n",
    "class_weight = 4\n",
    "lr = 0.1\n",
    "epoch_count = 50\n",
    "\n",
    "log_dir = 'runs/' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \\\n",
    "          f'_{ower_dataset}_emb-{emb_size}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Build train/valid DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ower_dir_path = Path('data/ower/ower-v3-fb-irt-3')\n",
    "ower_dir = OwerDir('OWER Dataset Directory', ower_dir_path, class_count, sent_count)\n",
    "ower_dir.check()\n",
    "\n",
    "train_set: List[Sample]\n",
    "valid_set: List[Sample]\n",
    "\n",
    "train_set, valid_set, _, vocab = ower_dir.read_datasets(vectors=vectors)\n",
    "\n",
    "#\n",
    "# Create dataloaders\n",
    "#\n",
    "\n",
    "def generate_batch(batch: List[Sample]) -> Tuple[Tensor, Tensor]:\n",
    "\n",
    "    _, gt_classes_batch, tok_lists_batch = zip(*batch)\n",
    "\n",
    "    cropped_sents_batch = [[sent[:sent_len]\n",
    "                            for sent in sents] for sents in tok_lists_batch]\n",
    "\n",
    "    padded_sents_batch = [[sent + [0] * (sent_len - len(sent))\n",
    "                           for sent in sents] for sents in cropped_sents_batch]\n",
    "\n",
    "    return tensor(padded_sents_batch), tensor(gt_classes_batch)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=generate_batch, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if vocab.vectors is None:\n",
    "    classifier = Classifier.from_random(len(vocab), emb_size, class_count)\n",
    "else:\n",
    "    classifier = Classifier.from_pre_trained(vocab, class_count)\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# criterion = MSELoss()\n",
    "criterion = BCEWithLogitsLoss()\n",
    "# criterion = BCEWithLogitsLoss(pos_weight=torch.tensor([class_weight] * class_count))\n",
    "\n",
    "# optimizer = SGD(classifier.parameters(), lr=lr)\n",
    "optimizer = Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "\n",
    "    #\n",
    "    # Train\n",
    "    #\n",
    "\n",
    "    # Train loss across all batches\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Valid gt/pred classes across all batches\n",
    "    train_gt_classes_stack: List[List[int]] = []\n",
    "    train_pred_classes_stack: List[List[int]] = []\n",
    "\n",
    "    for batch_idx, (sents_batch, gt_classes_batch) in enumerate(train_loader):\n",
    "        logits_batch = classifier(sents_batch)\n",
    "\n",
    "        loss = criterion(logits_batch, gt_classes_batch.float())\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred_classes_batch = (logits_batch > 0).int()\n",
    "\n",
    "        train_gt_classes_stack += gt_classes_batch.numpy().tolist()\n",
    "        train_pred_classes_stack += pred_classes_batch.numpy().tolist()\n",
    "        \n",
    "        #\n",
    "        # Print first batch\n",
    "        #\n",
    "\n",
    "        if batch_idx == 0:\n",
    "\n",
    "            dlb = logits_batch.detach().numpy()  # logits batch\n",
    "            dpb = pred_classes_batch.detach().numpy()  # predicted classes batch\n",
    "            dgb = gt_classes_batch.detach().numpy()  # ground truth classes batch\n",
    "            dsb = sents_batch.detach().numpy()  # sentences batch\n",
    "\n",
    "            df_cols = ['entity', 'logits', 'p', 'gt', 'sents']\n",
    "            df_data = [('foo', logits, pred_classes, classes, [[vocab.itos[tok] for tok in sent] for sent in sents])\n",
    "                       for logits, pred_classes, classes, sents in zip(dlb, dpb, dgb, dsb)]\n",
    "\n",
    "            df = pd.DataFrame(df_data, columns=df_cols)\n",
    "            display(df)\n",
    "\n",
    "    #\n",
    "    # Validate\n",
    "    #\n",
    "\n",
    "    # Valid loss across all batches\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    # Valid gt/pred classes across all batches\n",
    "    valid_gt_classes_stack: List[List[int]] = []\n",
    "    valid_pred_classes_stack: List[List[int]] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (sents_batch, gt_classes_batch) in enumerate(valid_loader):\n",
    "            logits_batch = classifier(sents_batch)\n",
    "\n",
    "            loss = criterion(logits_batch, gt_classes_batch.float())\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            pred_classes_batch = (logits_batch > 0).int()\n",
    "\n",
    "            valid_gt_classes_stack += gt_classes_batch.numpy().tolist()\n",
    "            valid_pred_classes_stack += pred_classes_batch.numpy().tolist()\n",
    "\n",
    "            #\n",
    "            # Print first batch\n",
    "            #\n",
    "\n",
    "            if batch_idx == 0:\n",
    "\n",
    "                dlb = logits_batch.detach().numpy()  # logits batch\n",
    "                dpb = pred_classes_batch.detach().numpy()  # predicted classes batch\n",
    "                dgb = gt_classes_batch.detach().numpy()  # ground truth classes batch\n",
    "                dsb = sents_batch.detach().numpy()  # sentences batch\n",
    "\n",
    "                df_cols = ['entity', 'logits', 'p', 'gt', 'sents']\n",
    "                df_data = [('foo', logits, pred_classes, classes, [[vocab.itos[tok] for tok in sent] for sent in sents])\n",
    "                           for logits, pred_classes, classes, sents in zip(dlb, dpb, dgb, dsb)]\n",
    "\n",
    "                df = pd.DataFrame(df_data, columns=df_cols)\n",
    "                display(df)\n",
    "\n",
    "    #\n",
    "    # Log\n",
    "    #\n",
    "\n",
    "    print(f'Epoch {epoch}')\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    writer.add_scalars('loss', {'train': train_loss, 'valid': valid_loss}, epoch)\n",
    "\n",
    "    # tps = train precisions, vps = valid precisions, etc.\n",
    "    tps = precision_score(train_gt_classes_stack, train_pred_classes_stack, average=None)\n",
    "    vps = precision_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None)\n",
    "    trs = recall_score(train_gt_classes_stack, train_pred_classes_stack, average=None)\n",
    "    vrs = recall_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None)\n",
    "    tfs = f1_score(train_gt_classes_stack, train_pred_classes_stack, average=None)\n",
    "    vfs = f1_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None)\n",
    "\n",
    "    # for i, (tp, vp, tr, vr, tf, vf) in enumerate(zip(tps, vps, trs, vrs, tfs, vfs)):\n",
    "    #     writer.add_scalars('precision', {f'train {i}': tp, f'valid {i}': vp}, epoch)\n",
    "    #     writer.add_scalars('recall', {f'train {i}': tr, f'valid {i}': vr}, epoch)\n",
    "    #     writer.add_scalars('f1', {f'train {i}': tf, f'valid {i}': vf}, epoch)\n",
    "\n",
    "    mean_train_precision = tps.mean()\n",
    "    mean_valid_precision = vps.mean()\n",
    "    mean_train_recall = trs.mean()\n",
    "    mean_valid_recall = vrs.mean()\n",
    "    mean_train_f1 = tfs.mean()\n",
    "    mean_valid_f1 = vfs.mean()\n",
    "\n",
    "    writer.add_scalars('precision', {f'train': mean_train_precision, f'valid': mean_valid_precision}, epoch)\n",
    "    writer.add_scalars('recall', {f'train': mean_train_recall, f'valid': mean_valid_recall}, epoch)\n",
    "    writer.add_scalars('f1', {f'train': mean_train_f1, f'valid': mean_valid_f1}, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Test\n",
    "\n",
    "## 4.1 Define test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        'ent': 1000,\n",
    "        'classes': [1, 0, 1, 0],  # married, male, American, actor\n",
    "        'sents': [\n",
    "            'Michelle is married',\n",
    "            'Michelle is female',\n",
    "            'Michelle is American'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'ent': 2000,\n",
    "        'classes': [1, 0, 0, 0],  # married, male, American, actor\n",
    "        'sents': [\n",
    "            'Angela is married',\n",
    "            'Angela is female',\n",
    "            'Angela is German'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return text.split()\n",
    "\n",
    "test_set = [Sample(\n",
    "    item['ent'],\n",
    "    item['classes'],\n",
    "    [[vocab[word] for word in tokenize(sent)] for sent in item['sents']]\n",
    ") for item in test_data]\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set), collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Forward test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for sents_batch, classes_batch in test_loader:\n",
    "        logits_batch = classifier(sents_batch)\n",
    "\n",
    "        print(logits_batch)\n",
    "\n",
    "        loss = criterion(logits_batch, classes_batch.float())\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "print(f'Test loss = {test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}