{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Softmax, EmbeddingBag, Linear"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import util"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def log_tensor(tensor: Tensor, title: str, labels: List[List[str]]):\n",
    "    pass\n",
    "\n",
    "%run util.ipynb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Declaration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def forward(embedding_bag: EmbeddingBag, class_embs: Tensor, linear: Linear, sents_batch: Tensor) -> Tensor:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Required globals"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size: int\n",
    "class_count: int\n",
    "emb_size: int\n",
    "sent_count: int\n",
    "sent_len: int\n",
    "\n",
    "class_labels: List[str]\n",
    "emb_labels: List[str]\n",
    "ent_class_labels: List[str]\n",
    "ent_labels: List[str]\n",
    "ent_sent_labels: List[str]\n",
    "mix_emb_labels: List[str]\n",
    "sent_labels: List[str]\n",
    "tok_labels: List[str]\n",
    "word_labels: List[str]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_sents(embedding_bag: EmbeddingBag, sents_batch: Tensor) -> Tensor:\n",
    "\n",
    "    #\n",
    "    # Flatten batch\n",
    "    #\n",
    "    # < sents_batch  (batch_size, sent_count, sent_len)\n",
    "    # > flat_sents   (batch_size * sent_count, sent_len)\n",
    "    #\n",
    "    \n",
    "    flat_sents = sents_batch.reshape(batch_size * sent_count, sent_len)\n",
    "\n",
    "    # log_tensor(sents_batch, 'sents_batch', [ent_labels, sent_labels, tok_labels])\n",
    "    # log_tensor(flat_sents, 'flat_sents', [ent_sent_labels, tok_labels])\n",
    "\n",
    "    #\n",
    "    # Embed sentences\n",
    "    #\n",
    "    # < embedding_bag.weight  (vocab_size, emb_size)\n",
    "    # < flat_sents            (batch_size * sent_count, sent_len)\n",
    "    # > flat_sent_embs        (batch_size * sent_count, emb_size)\n",
    "    #\n",
    "\n",
    "    flat_sent_embs = embedding_bag(flat_sents)\n",
    "\n",
    "    # log_tensor(embedding_bag.weight, 'embedding_bag.weight', [word_labels, emb_labels])\n",
    "    # log_tensor(flat_sents, 'flat_sents', [ent_sent_labels, tok_labels])\n",
    "    # log_tensor(flat_sent_embs, 'flat_sent_embs', [ent_sent_labels, emb_labels])\n",
    "\n",
    "    #\n",
    "    # Restore batch\n",
    "    #\n",
    "    # < flat_sent_embs   (batch_size * sent_count, emb_size)\n",
    "    # > sent_embs_batch  (batch_size, sent_count, emb_size)\n",
    "    #\n",
    "\n",
    "    sent_embs_batch = flat_sent_embs.reshape(batch_size, sent_count, emb_size)\n",
    "\n",
    "    # log_tensor(flat_sent_embs, 'flat_sent_embs', [ent_sent_labels, emb_labels])\n",
    "    # log_tensor(sent_embs_batch, 'sent_embs_batch', [ent_labels, sent_labels, emb_labels])\n",
    "\n",
    "    return sent_embs_batch\n",
    "\n",
    "def calc_atts(class_embs: Tensor, sent_embs_batch: Tensor) -> Tensor:\n",
    "\n",
    "    #\n",
    "    # Expand class embeddings for bmm()\n",
    "    #\n",
    "    # < class_embs        (class_count, emb_size)\n",
    "    # > class_embs_batch  (batch_size, class_count, emb_size)\n",
    "    #\n",
    "\n",
    "    class_embs_batch = class_embs.expand(batch_size, class_count, emb_size)\n",
    "\n",
    "    # log_tensor(class_embs, 'class_embs', [class_labels, emb_labels])\n",
    "    # log_tensor(class_embs_batch, 'class_embs_batch', [ent_labels, class_labels, emb_labels])\n",
    "\n",
    "    #\n",
    "    # Multiply each class with each sentence\n",
    "    #\n",
    "    # < class_embs_batch    (batch_size, class_count, emb_size)\n",
    "    # < sent_embs_batch     (batch_size, sent_count, emb_size)\n",
    "    # > atts_batch          (batch_size, class_count, sent_count)\n",
    "    #\n",
    "\n",
    "    atts_batch = torch.bmm(class_embs_batch, sent_embs_batch.transpose(1, 2))\n",
    "\n",
    "    # log_tensor(class_embs_batch, 'class_embs_batch', [ent_labels, class_labels, emb_labels])\n",
    "    # log_tensor(sent_embs_batch, 'sent_embs_batch', [ent_labels, sent_labels, emb_labels])\n",
    "    # log_tensor(atts_batch, 'atts_batch', [ent_labels, class_labels, sent_labels])\n",
    "\n",
    "    #\n",
    "    # Apply softmax over sentences\n",
    "    #\n",
    "    # < atts_batch      (batch_size, class_count, sent_count)\n",
    "    # > softs_batch     (batch_size, class_count, sent_count)\n",
    "    #\n",
    "\n",
    "    softs_batch = Softmax(dim=-1)(atts_batch)\n",
    "\n",
    "    # log_tensor(atts_batch, 'atts_batch', [ent_labels, class_labels, sent_labels])\n",
    "    # log_tensor(softs_batch, 'softs_batch', [ent_labels, class_labels, sent_labels])\n",
    "\n",
    "    return softs_batch\n",
    "\n",
    "def mix_sents(sent_embs_batch: Tensor, softs_batch: Tensor) -> Tensor:\n",
    "\n",
    "    #\n",
    "    # Repeat each batch slice class_count times\n",
    "    #\n",
    "    # < sent_embs_batch     (batch_size, sent_count, emb_size)\n",
    "    # > expaned_batch       (batch_size, class_count, sent_count, emb_size)\n",
    "    #\n",
    "\n",
    "    expaned_batch = sent_embs_batch.unsqueeze(1).expand(-1, class_count, -1, -1)\n",
    "\n",
    "    # log_tensor(sent_embs_batch, 'sent_embs_batch', [ent_labels, sent_labels, emb_labels])\n",
    "    # log_tensor(expaned_batch, 'expaned_batch', [ent_labels, class_labels, sent_labels, emb_labels])\n",
    "\n",
    "    #\n",
    "    # Flatten sentences for bmm()\n",
    "    #\n",
    "    # < expaned_batch   (batch_size, class_count, sent_count, emb_size)\n",
    "    # > flat_expanded   (batch_size * class_count, sent_count, emb_size)\n",
    "    #\n",
    "\n",
    "    flat_expanded = expaned_batch.reshape(-1, sent_count, emb_size)\n",
    "\n",
    "    # log_tensor(expaned_batch, 'expaned_batch', [ent_labels, class_labels, sent_labels, emb_labels])\n",
    "    # log_tensor(flat_expanded, 'flat_expanded', [ent_class_labels, sent_labels, emb_labels])\n",
    "\n",
    "    #\n",
    "    # Flatten attentions for bmm()\n",
    "    #\n",
    "    # < softs_batch     (batch_size, class_count, sent_count)\n",
    "    # > flat_softs      (batch_size * class_count, sent_count, 1)\n",
    "    #\n",
    "\n",
    "    flat_softs = softs_batch.reshape(batch_size * class_count, sent_count).unsqueeze(-1)\n",
    "\n",
    "    # log_tensor(softs_batch, 'softs_batch', [ent_labels, class_labels, sent_labels])\n",
    "    # log_tensor(flat_softs, 'flat_softs', [ent_class_labels, sent_labels, ['']])\n",
    "\n",
    "    #\n",
    "    # Multiply each sentence with each attention\n",
    "    #\n",
    "    # < flat_expanded  (batch_size * class_count, sent_count, emb_size)\n",
    "    # < flat_softs     (batch_size * class_count, sent_count, 1)\n",
    "    # > flat_mixes     (batch_size * class_count, emb_size)\n",
    "    #\n",
    "\n",
    "    flat_mixes = torch.bmm(flat_expanded.transpose(1, 2), flat_softs).squeeze(-1)\n",
    "\n",
    "    # log_tensor(flat_expanded, 'flat_expanded', [ent_class_labels, sent_labels, emb_labels])\n",
    "    # log_tensor(flat_softs, 'flat_softs', [ent_class_labels, sent_labels, ['']])\n",
    "    # log_tensor(flat_mixes, 'flat_mixes', [ent_class_labels, emb_labels])\n",
    "\n",
    "    #\n",
    "    # Restore batch\n",
    "    #\n",
    "    # < flat_mixes   (batch_size * class_count, emb_size)\n",
    "    # > mixes_batch  (batch_size, class_count, emb_size)\n",
    "    #\n",
    "\n",
    "    mixes_batch = flat_mixes.reshape(batch_size, class_count, emb_size)\n",
    "\n",
    "    # log_tensor(flat_mixes, 'flat_mixes', [ent_class_labels, emb_labels])\n",
    "    # log_tensor(mixes_batch, 'mixes_batch', [ent_labels, class_labels, emb_labels])\n",
    "\n",
    "    return mixes_batch\n",
    "\n",
    "def forward(embedding_bag: EmbeddingBag, class_embs: Tensor, linear: Linear, sents_batch: Tensor) -> Tensor:\n",
    "\n",
    "    #\n",
    "    # Embed sentences\n",
    "    #\n",
    "    # < embedding_bag.weight  (vocab_size, emb_size)\n",
    "    # < sents_batch           (batch_size, sent_count, sent_len)\n",
    "    # > sent_embs_batch       (batch_size, sent_count, emb_size)\n",
    "    #\n",
    "\n",
    "    sent_embs_batch = embed_sents(embedding_bag, sents_batch)\n",
    "\n",
    "    # log_tensor(embedding_bag.weight, 'embedding_bag.weight', [word_labels, emb_labels])\n",
    "    # log_tensor(sents_batch, 'sents_batch', [ent_labels, sent_labels, tok_labels])\n",
    "    # log_tensor(sent_embs_batch, 'sent_embs_batch', [ent_labels, sent_labels, emb_labels])\n",
    "\n",
    "    #\n",
    "    # Calculate attentions (which class matches which sentences)\n",
    "    #\n",
    "    # < class_embs       (class_count, emb_size)\n",
    "    # < sent_embs_batch  (batch_size, sent_count, emb_size)\n",
    "    # > atts_batch       (batch_size, class_count, sent_count)\n",
    "    #\n",
    "\n",
    "    atts_batch = calc_atts(class_embs, sent_embs_batch)\n",
    "\n",
    "    # log_tensor(class_embs, 'class_embs', [class_labels, emb_labels])\n",
    "    # log_tensor(sent_embs_batch, 'sent_embs_batch', [ent_labels, sent_labels, emb_labels])\n",
    "    # log_tensor(atts_batch, 'atts_batch', [ent_labels, class_labels, sent_labels])\n",
    "\n",
    "    #\n",
    "    # For each class, mix sentences (as per class' attentions to sentences)\n",
    "    #\n",
    "    # < sent_embs_batch  (batch_size, sent_count, emb_size)\n",
    "    # < atts_batch       (batch_size, class_count, sent_count)\n",
    "    # > mixes_batch      (batch_size, class_count, emb_size)\n",
    "    #\n",
    "\n",
    "    mixes_batch = mix_sents(sent_embs_batch, atts_batch)\n",
    "\n",
    "    # log_tensor(sent_embs_batch, 'sent_embs_batch', [ent_labels, sent_labels, emb_labels])\n",
    "    # log_tensor(atts_batch, 'atts_batch', [ent_labels, class_labels, sent_labels])\n",
    "    # log_tensor(mixes_batch, 'mixes_batch', [ent_labels, class_labels, emb_labels])\n",
    "\n",
    "    #\n",
    "    # Concatenate mixes\n",
    "    #\n",
    "    # < weighted_batch  (batch_size, class_count, emb_size)\n",
    "    # > concat_mixes_batch  (batch_size, class_count * emb_size)\n",
    "    #\n",
    "\n",
    "    concat_mixes_batch = mixes_batch.reshape(batch_size, class_count * emb_size)\n",
    "\n",
    "    # log_tensor(mixes_batch, 'mixes_batch', [ent_labels, class_labels, emb_labels])\n",
    "    # log_tensor(concat_mixes_batch, 'concat_mixes_batch', [ent_labels, mix_emb_labels])\n",
    "\n",
    "    #\n",
    "    # Push concatenated mixes through linear layer\n",
    "    #\n",
    "    # < concat_mixes_batch  (batch_size, class_count * emb_size)\n",
    "    # > logits_batch        (batch_size, class_count)\n",
    "    #\n",
    "\n",
    "    logits_batch = linear(concat_mixes_batch)\n",
    "\n",
    "    # log_tensor(concat_mixes_batch, 'concat_mixes_batch', [ent_labels, mix_emb_labels])\n",
    "    # log_tensor(logits_batch, 'logits_batch', [ent_labels, class_labels])\n",
    "\n",
    "    return logits_batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}