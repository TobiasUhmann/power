{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feb 23\n",
    "\n",
    "Notebook 1, \"Simple Positive Case\" does not deliver very helpful results.\n",
    "It is not clear whether the prepared classifier weights represent the\n",
    "classifier state after being learned.\n",
    "\n",
    "A better test might be starting out with a classifier with random weights\n",
    "and trying to overfit on a single sample. The expected result would be\n",
    "learned weights that are similar to the preparation in notebook 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.lib.pretty import pretty\n",
    "\n",
    "def log_tensor(tensor, title, labels, vmin=None, vmax=None):\n",
    "    pass\n",
    "\n",
    "%run util.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    { 'ent': 123, 'classes': [1, 1, 1], 'sents': ['married married married', 'male male male', 'American American American'] },\n",
    "    { 'ent': 123, 'classes': [1, 1, 1], 'sents': ['married married married', 'male male male', 'American American American'] },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Pre-processing\n",
    "\n",
    "## 2.1 Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "words = [word for ent in data for sent in ent['sents'] for word in tokenize(sent)]\n",
    "vocab = Vocab(Counter(words))\n",
    "\n",
    "print(pretty(vocab.stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transform data\n",
    "\n",
    "Map words to tokens and create tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "\n",
    "sents_batch = tensor([[[vocab[word] for word in tokenize(sent)] for sent in ent['sents']] for ent in data])\n",
    "classes_batch = torch.tensor([ent['classes'] for ent in data])\n",
    "\n",
    "assert len(sents_batch) == len(classes_batch)\n",
    "\n",
    "batch_size = len(sents_batch)\n",
    "sent_count = 3\n",
    "sent_len = 3\n",
    "\n",
    "ent_labels = [f'ent {i}' for i in range(batch_size)]\n",
    "sent_labels = [f'sent {i}' for i in range(sent_count)]\n",
    "tok_labels = [f'tok {i}' for i in range(sent_len)]\n",
    "\n",
    "log_tensor(sents_batch, 'sents_batch', [ent_labels, sent_labels, tok_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3 Prepare classifier\n",
    "\n",
    "## 3.1 Prepare EmbeddingBag\n",
    "\n",
    "Create and prepare an `EmbeddingBag` with randomly distributed token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import EmbeddingBag\n",
    "from torch import tensor\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "assert vocab_size == 5\n",
    "\n",
    "emb_size = 4\n",
    "\n",
    "embedding_bag = EmbeddingBag(num_embeddings=vocab_size, embedding_dim=emb_size)\n",
    "\n",
    "log_output = 1\n",
    "\n",
    "word_labels = ['<unk>', '<pad>', 'married', 'male', 'American']\n",
    "emb_labels = [f'emb {i}' for i in range(emb_size)]\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(embedding_bag.weight, 'embedding_bag.weight', [word_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Prepare class embeddings\n",
    "\n",
    "Create randomly initialized class embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_count = 3\n",
    "\n",
    "class_embs = torch.rand((class_count, emb_size), requires_grad=True)\n",
    "\n",
    "log_output = 1\n",
    "\n",
    "class_labels = ['married', 'male', 'American']\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(class_embs, 'class_embs', [class_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Prepare linear layer\n",
    "\n",
    "Create a randomly initialized linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "linear = Linear(class_count * emb_size, class_count)\n",
    "\n",
    "log_output = 1\n",
    "\n",
    "mix_emb_labels = [f'mix {i} / emb {j}' for i in range(class_count) for j in range(emb_size)]\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(linear.weight.data, 'linear.weight.data', [class_labels, mix_emb_labels])\n",
    "    log_tensor(linear.bias.data, 'linear.bias.data', [class_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Forward & Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# 4.1 Embed sentences\n",
    "#\n",
    "\n",
    "#\n",
    "# Flatten batch\n",
    "#\n",
    "# < sents_batch  (batch_size, sent_count, sent_len)\n",
    "# > flat_sents   (batch_size * sent_count, sent_len)\n",
    "#\n",
    "\n",
    "flat_sents = sents_batch.reshape(batch_size * sent_count, sent_len)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "ent_sent_labels = [f'ent {i} / sent {j}' for i in range(batch_size) for j in range(sent_count)]\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(sents_batch, 'sents_batch', [ent_labels, sent_labels, tok_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(flat_sents, 'flat_sents', [ent_sent_labels, tok_labels])\n",
    "\n",
    "#\n",
    "# Embed sentences\n",
    "#\n",
    "# < embedding_bag.weight  (vocab_size, emb_size)\n",
    "# < flat_sents            (batch_size * sent_count, sent_len)\n",
    "# > flat_sent_embs        (batch_size * sent_count, emb_size)\n",
    "#\n",
    "\n",
    "flat_sent_embs = embedding_bag(flat_sents)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(embedding_bag.weight, 'embedding_bag.weight', [word_labels, emb_labels])\n",
    "    log_tensor(flat_sents, 'flat_sents', [ent_sent_labels, tok_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(flat_sent_embs, 'flat_sent_embs', [ent_sent_labels, emb_labels])\n",
    "\n",
    "#\n",
    "# Restore batch\n",
    "#\n",
    "# < flat_sent_embs   (batch_size * sent_count, emb_size)\n",
    "# > sent_embs_batch  (batch_size, sent_count, emb_size)\n",
    "#\n",
    "\n",
    "sent_embs_batch = flat_sent_embs.reshape(batch_size, sent_count, emb_size)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 1\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(flat_sent_embs, 'flat_sent_embs', [ent_sent_labels, emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(sent_embs_batch, 'sent_embs_batch', [ent_labels, sent_labels, emb_labels])\n",
    "\n",
    "#\n",
    "# 4.2 Calc attentions\n",
    "#\n",
    "\n",
    "#\n",
    "# Expand class embeddings for bmm()\n",
    "#\n",
    "# < class_embs        (class_count, emb_size)\n",
    "# > class_embs_batch  (batch_size, class_count, emb_size)\n",
    "#\n",
    "\n",
    "class_embs_batch = class_embs.expand(batch_size, class_count, emb_size)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(class_embs, 'class_embs', [class_labels, emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(class_embs_batch, 'class_embs_batch', [ent_labels, class_labels, emb_labels])\n",
    "\n",
    "#\n",
    "# Multiply each class with each sentence\n",
    "#\n",
    "# < class_embs_batch    (batch_size, class_count, emb_size)\n",
    "# < sent_embs_batch     (batch_size, sent_count, emb_size)\n",
    "# > atts_batch          (batch_size, class_count, sent_count)\n",
    "#\n",
    "\n",
    "atts_batch = torch.bmm(class_embs_batch, sent_embs_batch.transpose(1, 2))\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(class_embs_batch, 'class_embs_batch', [ent_labels, class_labels, emb_labels])\n",
    "    log_tensor(sent_embs_batch, 'sent_embs_batch', [ent_labels, sent_labels, emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(atts_batch, 'atts_batch', [ent_labels, class_labels, sent_labels])\n",
    "\n",
    "#\n",
    "# Apply softmax over sentences\n",
    "#\n",
    "# < atts_batch      (batch_size, class_count, sent_count)\n",
    "# > softs_batch     (batch_size, class_count, sent_count)\n",
    "#\n",
    "\n",
    "from torch.nn import Softmax\n",
    "\n",
    "softs_batch = Softmax(dim=-1)(atts_batch)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 1\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(atts_batch, 'atts_batch', [ent_labels, class_labels, sent_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(softs_batch, 'softs_batch', [ent_labels, class_labels, sent_labels])\n",
    "\n",
    "#\n",
    "# 4.3 Weight and mix sentences\n",
    "#\n",
    "\n",
    "#\n",
    "# Repeat each batch slice class_count times\n",
    "#\n",
    "# < sent_embs_batch     (batch_size, sent_count, emb_size)\n",
    "# > expaned_batch       (batch_size, class_count, sent_count, emb_size)\n",
    "#\n",
    "\n",
    "expaned_batch = sent_embs_batch.unsqueeze(1).expand(-1, class_count, -1, -1)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(sent_embs_batch, 'sent_embs_batch', [ent_labels, sent_labels, emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(expaned_batch, 'expaned_batch', [ent_labels, class_labels, sent_labels, emb_labels])\n",
    "\n",
    "#\n",
    "# Flatten sentences for bmm()\n",
    "#\n",
    "# < expaned_batch   (batch_size, class_count, sent_count, emb_size)\n",
    "# > flat_expanded   (batch_size * class_count, sent_count, emb_size)\n",
    "#\n",
    "\n",
    "flat_expanded = expaned_batch.reshape(-1, sent_count, emb_size)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "ent_class_labels = [f'ent {i} / class {j}' for i in range(batch_size) for j in range(class_count)]\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(expaned_batch, 'expaned_batch', [ent_labels, class_labels, sent_labels, emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(flat_expanded, 'flat_expanded', [ent_class_labels, sent_labels, emb_labels])\n",
    "\n",
    "#\n",
    "# Flatten attentions for bmm()\n",
    "#\n",
    "# < softs_batch     (batch_size, class_count, sent_count)\n",
    "# > flat_softs      (batch_size * class_count, sent_count, 1)\n",
    "#\n",
    "\n",
    "flat_softs = softs_batch.reshape(batch_size * class_count, sent_count).unsqueeze(-1)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(softs_batch, 'softs_batch', [ent_labels, class_labels, sent_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(flat_softs, 'flat_softs', [ent_class_labels, sent_labels, ['']])\n",
    "\n",
    "#\n",
    "# Multiply each sentence with each attention\n",
    "#\n",
    "# < flat_expanded   (batch_size * class_count, sent_count, emb_size)\n",
    "# < flat_softs      (batch_size * class_count, sent_count, 1)\n",
    "# > flat_weighted   (batch_size * class_count, emb_size)\n",
    "#\n",
    "\n",
    "flat_weighted = torch.bmm(flat_expanded.transpose(1, 2), flat_softs).squeeze(-1)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(flat_expanded, 'flat_expanded', [ent_class_labels, sent_labels, emb_labels])\n",
    "    log_tensor(flat_softs, 'flat_softs', [ent_class_labels, sent_labels, ['']])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(flat_weighted, 'flat_weighted', [ent_class_labels, emb_labels])\n",
    "\n",
    "#\n",
    "# Restore batch\n",
    "#\n",
    "# < flat_weighted   (batch_size * class_count, emb_size)\n",
    "# > weighted_batch  (batch_size, class_count, emb_size)\n",
    "#\n",
    "\n",
    "weighted_batch = flat_weighted.reshape(batch_size, class_count, emb_size)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 1\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(flat_weighted, 'flat_weighted', [ent_class_labels, emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(weighted_batch, 'weighted_batch', [ent_labels, class_labels, emb_labels])\n",
    "\n",
    "#\n",
    "# 4.4 Linear layer\n",
    "#\n",
    "\n",
    "#\n",
    "# Concatenate mixes\n",
    "#\n",
    "# < weighted_batch  (batch_size, class_count, emb_size)\n",
    "# > concat_mixes_batch  (batch_size, class_count * emb_size)\n",
    "#\n",
    "\n",
    "concat_mixes_batch = weighted_batch.reshape(batch_size, class_count * emb_size)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 1\n",
    "\n",
    "mix_emb_labels = [f'mix {i} / emb {j}' for i in range(class_count) for j in range(emb_size)]\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(weighted_batch, 'weighted_batch', [ent_labels, class_labels, emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(concat_mixes_batch, 'concat_mixes_batch', [ent_labels, mix_emb_labels])\n",
    "\n",
    "#\n",
    "# Push concatenated mixes through linear layer\n",
    "#\n",
    "# < concat_mixes_batch  (batch_size, class_count * emb_size)\n",
    "# > logits_batch        (batch_size, class_count)\n",
    "#\n",
    "\n",
    "logits_batch = linear(concat_mixes_batch)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 1\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(concat_mixes_batch, 'concat_mixes_batch', [ent_labels, mix_emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(logits_batch, 'logits_batch', [ent_labels, class_labels], vmin=-1, vmax=1)\n",
    "\n",
    "#\n",
    "# 4.5 Loss\n",
    "#\n",
    "\n",
    "#\n",
    "# Push concatenated mixes through linear layer\n",
    "#\n",
    "# < concat_mixes_batch  (batch_size, class_count * emb_size)\n",
    "# > logits_batch        (batch_size, class_count)\n",
    "#\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "criterion = BCEWithLogitsLoss(pos_weight=torch.tensor([80] * class_count))\n",
    "# criterion = torch.nn.MSELoss()\n",
    "\n",
    "loss = criterion(logits_batch, classes_batch.float())\n",
    "\n",
    "log_input = 0\n",
    "log_output = 1\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(logits_batch, 'logits_batch', [ent_labels, class_labels], vmin=-1, vmax=1)\n",
    "    log_tensor(classes_batch, 'classes_batch', [ent_labels, class_labels], vmin=-1, vmax=1)\n",
    "\n",
    "log_tensor(loss, 'loss', [])\n",
    "\n",
    "#\n",
    "# 4.6 Backward\n",
    "#\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam([embedding_bag.weight, class_embs, linear.weight, linear.bias], lr=0.1)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}