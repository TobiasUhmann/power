{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feb 25\n",
    "\n",
    "For some reason the valid loss does not decrease significantly\n",
    "on the attention model. Compare with a baseline that does not\n",
    "include the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from IPython.lib.pretty import pretty\n",
    "from torch import tensor, Tensor\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data import Field, TabularDataset\n",
    "\n",
    "from classifier import Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Build train/valid DataLoaders\n",
    "\n",
    "## 1.1 Read Sample TSVs into TabularDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_count = 4\n",
    "sent_count = 3\n",
    "\n",
    "#\n",
    "# Define columns for subsequent read into TabularDatasets\n",
    "#\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return text.split()\n",
    "\n",
    "ent_field = ('ent', Field(sequential=False, use_vocab=False))\n",
    "\n",
    "class_fields = [(f'class_{i}', Field(sequential=False, use_vocab=False))\n",
    "                for i in range(class_count)]\n",
    "\n",
    "sent_fields = [(f'sent_{i}', Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True))\n",
    "               for i in range(sent_count)]\n",
    "\n",
    "fields = [ent_field] + class_fields + sent_fields\n",
    "\n",
    "#\n",
    "# Read Train Samples TSV into TabularDataset\n",
    "#\n",
    "\n",
    "train_samples_tsv = 'data/ower-v3-fb-3/train.tsv'\n",
    "valid_samples_tsv = 'data/ower-v3-fb-3/valid.tsv'\n",
    "# test_samples_tsv = 'data/ower-v3-fb-3/test.tsv'\n",
    "\n",
    "raw_train_set = TabularDataset(train_samples_tsv, 'tsv', fields, skip_header=True)\n",
    "raw_valid_set = TabularDataset(valid_samples_tsv, 'tsv', fields, skip_header=True)\n",
    "# raw_test_set = TabularDataset(test_samples_tsv, 'tsv', fields, skip_header=True)\n",
    "\n",
    "for i in range(3):\n",
    "    row = raw_train_set[i]\n",
    "\n",
    "    print('entity', row.ent)\n",
    "    print('classes: married = {}, male = {}, American = {}, actor = {}'.format(\n",
    "        row.class_0, row.class_1, row.class_2, row.class_3))\n",
    "    print(row.sent_0)\n",
    "    print(row.sent_1)\n",
    "    print(row.sent_2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2 Build vocab on train data\n",
    "\n",
    "The docs only show how to build a vocab over a single column. Therefore, the vocab\n",
    "is built over the first sentence column only atm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "first_sent_field = sent_fields[0][1]\n",
    "first_sent_field.build_vocab(raw_train_set)\n",
    "vocab = first_sent_field.vocab\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab_size)\n",
    "print(vocab.itos[:10])\n",
    "print(vocab.itos[vocab_size//2:vocab_size//2+10])\n",
    "print(vocab.itos[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.3 Transfor each TabularDataset -> List[Sample]\n",
    "\n",
    "Parse texts from datasets, map words -> tokens (IDs) using vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sample:\n",
    "    ent: int\n",
    "    classes: List[int]\n",
    "    sents: List[List[int]]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter((self.ent, self.classes, self.sents))\n",
    "\n",
    "\n",
    "def transform(raw_set: TabularDataset) -> List[Sample]:\n",
    "    return [Sample(\n",
    "        int(getattr(row, 'ent')),\n",
    "        [int(getattr(row, f'class_{i}')) for i in range(class_count)],\n",
    "        [[vocab[token] for token in getattr(row, f'sent_{i}')] for i in range(sent_count)]\n",
    "    ) for row in raw_set]\n",
    "\n",
    "\n",
    "train_set = transform(raw_train_set)\n",
    "valid_set = transform(raw_valid_set)\n",
    "# test_set = transform(raw_test_set)\n",
    "\n",
    "print('First training sample:')\n",
    "print(pretty(train_set[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.4 Build DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch: List[Sample]) -> Tuple[Tensor, Tensor]:\n",
    "\n",
    "    _ent, classes_batch, sents_batch = zip(*batch)\n",
    "\n",
    "    cropped_sents_batch = [[sent[:sent_len]\n",
    "                            for sent in sents] for sents in sents_batch]\n",
    "\n",
    "    padded_sents_batch = [[sent + [0] * (sent_len - len(sent))\n",
    "                           for sent in sents] for sents in cropped_sents_batch]\n",
    "\n",
    "    return tensor(padded_sents_batch), tensor(classes_batch)\n",
    "\n",
    "batch_size = 1024\n",
    "sent_len = 64\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=generate_batch, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, collate_fn=generate_batch)\n",
    "# test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2 Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emb_size = 256\n",
    "\n",
    "classifier = Classifier(vocab_size, emb_size, class_count)\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# criterion = MSELoss()\n",
    "# criterion = BCEWithLogitsLoss()\n",
    "criterion = BCEWithLogitsLoss(pos_weight=torch.tensor([80] * class_count))\n",
    "\n",
    "# optimizer = SGD(classifier.parameters(), lr=0.1)\n",
    "optimizer = Adam(classifier.parameters(), lr=0.1)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "\n",
    "    #\n",
    "    # Train\n",
    "    #\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for sents_batch, classes_batch in tqdm(train_loader, leave=False):\n",
    "        logits_batch = classifier(sents_batch)\n",
    "\n",
    "        loss = criterion(logits_batch, classes_batch.float())\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    #\n",
    "    # Validate\n",
    "    #\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for sents_batch, classes_batch in tqdm(valid_loader, leave=False):\n",
    "            logits_batch = classifier(sents_batch)\n",
    "\n",
    "            loss = criterion(logits_batch, classes_batch.float())\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    #\n",
    "    # Log\n",
    "    #\n",
    "\n",
    "    print(f'Epoch {epoch}: Train loss = {train_loss}, valid loss = {valid_loss}')\n",
    "    \n",
    "    writer.add_scalars('loss', {'train': train_loss, 'valid': valid_loss}, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4 Test\n",
    "\n",
    "## 4.1 Define test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        'ent': 1000,\n",
    "        'classes': [1, 0, 1, 0],  # married, male, American, actor\n",
    "        'sents': [\n",
    "            'Michelle is married',\n",
    "            'Michelle is female',\n",
    "            'Michelle is American'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'ent': 2000,\n",
    "        'classes': [1, 0, 0, 0],  # married, male, American, actor\n",
    "        'sents': [\n",
    "            'Angela is married',\n",
    "            'Angela is female',\n",
    "            'Angela is German'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "test_set = [Sample(\n",
    "    item['ent'],\n",
    "    item['classes'],\n",
    "    [[vocab[word] for word in tokenize(sent)] for sent in item['sents']]\n",
    ") for item in test_data]\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set), collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Forward test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for sents_batch, classes_batch in test_loader:\n",
    "        logits_batch = classifier(sents_batch)\n",
    "        \n",
    "        print(logits_batch)\n",
    "\n",
    "        loss = criterion(logits_batch, classes_batch.float())\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "print(f'Test loss = {test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}