{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mar 1\n",
    "\n",
    "Measure and plot precision, recall and F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.display import display\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch import tensor, Tensor\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data import Field, TabularDataset\n",
    "\n",
    "from attention_classifier import Classifier\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ower_dataset = 'ower-v3-fb-irt-3'\n",
    "class_count = 4\n",
    "sent_count = 3\n",
    "\n",
    "# vectors = None\n",
    "# vectors = 'charngram.100d'\n",
    "# vectors = 'fasttext.en.300d'\n",
    "# vectors = 'fasttext.simple.300d'\n",
    "# vectors = 'glove.42B.300d'\n",
    "# vectors = 'glove.840B.300d'\n",
    "# vectors = 'glove.twitter.27B.25d'\n",
    "# vectors = 'glove.twitter.27B.50d'\n",
    "# vectors = 'glove.twitter.27B.100d'\n",
    "vectors = 'glove.twitter.27B.200d'\n",
    "# vectors = 'glove.6B.50d'\n",
    "# vectors = 'glove.6B.100d'\n",
    "# vectors = 'glove.6B.200d'\n",
    "# vectors = 'glove.6B.300d'\n",
    "\n",
    "emb_size = None\n",
    "# emb_size = 200\n",
    "\n",
    "batch_size = 1024\n",
    "sent_len = 64\n",
    "\n",
    "class_weight = 4\n",
    "lr = 0.1\n",
    "epoch_count = 50\n",
    "\n",
    "log_dir = 'runs/' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \\\n",
    "          f'_{ower_dataset}' + f'_emb-{emb_size}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Build train/valid DataLoaders\n",
    "\n",
    "## 1.1 Read Sample TSVs into TabularDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Define columns for subsequent read into TabularDatasets\n",
    "#\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return text.split()\n",
    "\n",
    "raw_ent_field = Field(sequential=False, use_vocab=False)\n",
    "raw_class_field = Field(sequential=False, use_vocab=False)\n",
    "raw_sent_field = Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True)\n",
    "\n",
    "ent_field = ('ent', raw_ent_field)\n",
    "class_fields = [(f'class_{i}', raw_class_field) for i in range(class_count)]\n",
    "sent_fields = [(f'sent_{i}', raw_sent_field) for i in range(sent_count)]\n",
    "\n",
    "fields = [ent_field] + class_fields + sent_fields\n",
    "\n",
    "#\n",
    "# Read Train Samples TSV into TabularDataset\n",
    "#\n",
    "\n",
    "train_samples_tsv = f'data/{ower_dataset}/train.tsv'\n",
    "valid_samples_tsv = f'data/{ower_dataset}/valid.tsv'\n",
    "\n",
    "raw_train_set = TabularDataset(train_samples_tsv, 'tsv', fields, skip_header=True)\n",
    "raw_valid_set = TabularDataset(valid_samples_tsv, 'tsv', fields, skip_header=True)\n",
    "\n",
    "#\n",
    "# Print some samples\n",
    "#\n",
    "\n",
    "tab_cols = ['ent'] + [f'class_{i}' for i in range(class_count)] + [f'sent_{i}' for i in range(sent_count)]\n",
    "\n",
    "train_tab_data = [[getattr(row, col) for col in tab_cols] for row in raw_train_set[:20]]\n",
    "display(pd.DataFrame(train_tab_data, columns=tab_cols))\n",
    "\n",
    "valid_tab_data = [[getattr(row, col) for col in tab_cols] for row in raw_valid_set[:20]]\n",
    "display(pd.DataFrame(valid_tab_data, columns=tab_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Build vocab on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "raw_sent_field.build_vocab(raw_train_set, vectors=vectors)\n",
    "vocab = raw_sent_field.vocab\n",
    "\n",
    "#\n",
    "# Print some samples\n",
    "#\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab_size)\n",
    "print(vocab.itos[:10])\n",
    "print(vocab.itos[vocab_size//2:vocab_size//2+10])\n",
    "print(vocab.itos[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Transfor each TabularDataset -> List[Sample]\n",
    "\n",
    "Parse texts from datasets, map words -> tokens (IDs) using vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sample:\n",
    "    ent: int\n",
    "    classes: List[int]\n",
    "    sents: List[List[int]]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter((self.ent, self.classes, self.sents))\n",
    "\n",
    "\n",
    "def transform(raw_set: TabularDataset) -> List[Sample]:\n",
    "    return [Sample(\n",
    "        int(getattr(row, 'ent')),\n",
    "        [int(getattr(row, f'class_{i}')) for i in range(class_count)],\n",
    "        [[vocab[token] for token in getattr(row, f'sent_{i}')] for i in range(sent_count)]\n",
    "    ) for row in raw_set]\n",
    "\n",
    "\n",
    "train_set = transform(raw_train_set)\n",
    "valid_set = transform(raw_valid_set)\n",
    "\n",
    "#\n",
    "# Print some samples\n",
    "#\n",
    "\n",
    "tab_cols = ['ent', 'classes', 'sents']\n",
    "tab_data = [[getattr(row, col) for col in tab_cols] for row in train_set[:3]]\n",
    "\n",
    "pd.DataFrame(tab_data, columns=tab_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Build DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch: List[Sample]) -> Tuple[Tensor, Tensor]:\n",
    "\n",
    "    _ent, classes_batch, sents_batch = zip(*batch)\n",
    "\n",
    "    cropped_sents_batch = [[sent[:sent_len]\n",
    "                            for sent in sents] for sents in sents_batch]\n",
    "\n",
    "    padded_sents_batch = [[sent + [0] * (sent_len - len(sent))\n",
    "                           for sent in sents] for sents in cropped_sents_batch]\n",
    "\n",
    "    return tensor(padded_sents_batch), tensor(classes_batch)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=generate_batch, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if vocab.vectors is None:\n",
    "    classifier = Classifier.from_random(vocab, emb_size, class_count)\n",
    "else:\n",
    "    classifier = Classifier.from_pre_trained(vocab, class_count)\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# criterion = MSELoss()\n",
    "# criterion = BCEWithLogitsLoss()\n",
    "criterion = BCEWithLogitsLoss(pos_weight=torch.tensor([class_weight] * class_count))\n",
    "\n",
    "# optimizer = SGD(classifier.parameters(), lr=lr)\n",
    "optimizer = Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "\n",
    "    #\n",
    "    # Train\n",
    "    #\n",
    "\n",
    "    # Train loss across all batches\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Valid gt/pred classes across all batches\n",
    "    train_gt_classes_stack: List[List[int]] = []\n",
    "    train_pred_classes_stack: List[List[int]] = []\n",
    "\n",
    "    for batch_idx, (sents_batch, gt_classes_batch) in enumerate(train_loader):\n",
    "        logits_batch = classifier(sents_batch)\n",
    "\n",
    "        loss = criterion(logits_batch, gt_classes_batch.float())\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred_classes_batch = (logits_batch > 0).int()\n",
    "\n",
    "        train_gt_classes_stack += gt_classes_batch.numpy().tolist()\n",
    "        train_pred_classes_stack += pred_classes_batch.numpy().tolist()\n",
    "        \n",
    "        #\n",
    "        # Print first batch\n",
    "        #\n",
    "\n",
    "        if batch_idx == 0:\n",
    "\n",
    "            dlb = logits_batch.detach().numpy()  # logits batch\n",
    "            dpb = pred_classes_batch.detach().numpy()  # predicted classes batch\n",
    "            dgb = gt_classes_batch.detach().numpy()  # ground truth classes batch\n",
    "            dsb = sents_batch.detach().numpy()  # sentences batch\n",
    "\n",
    "            df_cols = ['entity', 'logits', 'p', 'gt', 'sents']\n",
    "            df_data = [('foo', logits, pred_classes, classes, [[vocab.itos[tok] for tok in sent] for sent in sents])\n",
    "                       for logits, pred_classes, classes, sents in zip(dlb, dpb, dgb, dsb)]\n",
    "\n",
    "            df = pd.DataFrame(df_data, columns=df_cols)\n",
    "            display(df)\n",
    "\n",
    "    #\n",
    "    # Validate\n",
    "    #\n",
    "\n",
    "    # Valid loss across all batches\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    # Valid gt/pred classes across all batches\n",
    "    valid_gt_classes_stack: List[List[int]] = []\n",
    "    valid_pred_classes_stack: List[List[int]] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (sents_batch, gt_classes_batch) in enumerate(valid_loader):\n",
    "            logits_batch = classifier(sents_batch)\n",
    "\n",
    "            loss = criterion(logits_batch, gt_classes_batch.float())\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            pred_classes_batch = (logits_batch > 0).int()\n",
    "\n",
    "            valid_gt_classes_stack += gt_classes_batch.numpy().tolist()\n",
    "            valid_pred_classes_stack += pred_classes_batch.numpy().tolist()\n",
    "\n",
    "            #\n",
    "            # Print first batch\n",
    "            #\n",
    "\n",
    "            if batch_idx == 0:\n",
    "\n",
    "                dlb = logits_batch.detach().numpy()  # logits batch\n",
    "                dpb = pred_classes_batch.detach().numpy()  # predicted classes batch\n",
    "                dgb = gt_classes_batch.detach().numpy()  # ground truth classes batch\n",
    "                dsb = sents_batch.detach().numpy()  # sentences batch\n",
    "\n",
    "                df_cols = ['entity', 'logits', 'p', 'gt', 'sents']\n",
    "                df_data = [('foo', logits, pred_classes, classes, [[vocab.itos[tok] for tok in sent] for sent in sents])\n",
    "                           for logits, pred_classes, classes, sents in zip(dlb, dpb, dgb, dsb)]\n",
    "\n",
    "                df = pd.DataFrame(df_data, columns=df_cols)\n",
    "                display(df)\n",
    "\n",
    "    #\n",
    "    # Log\n",
    "    #\n",
    "\n",
    "    print(f'Epoch {epoch}')\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    print(f'    Loss:  train = {train_loss:.2f}, valid = {valid_loss:.2f}')\n",
    "    writer.add_scalars('loss', {'train': train_loss, 'valid': valid_loss}, epoch)\n",
    "\n",
    "    # tps = train precisions, vps = valid precisions, etc.\n",
    "    tps = precision_score(train_gt_classes_stack, train_pred_classes_stack, average=None)\n",
    "    vps = precision_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None)\n",
    "    trs = recall_score(train_gt_classes_stack, train_pred_classes_stack, average=None)\n",
    "    vrs = recall_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None)\n",
    "    tfs = f1_score(train_gt_classes_stack, train_pred_classes_stack, average=None)\n",
    "    vfs = f1_score(valid_gt_classes_stack, valid_pred_classes_stack, average=None)\n",
    "\n",
    "    for i, (tp, vp, tr, vr, tf, vf) in enumerate(zip(tps, vps, trs, vrs, tfs, vfs)):\n",
    "        print(f'    Precision {i}:  train = {tp:.2f}, valid = {vp:.2f}')\n",
    "        print(f'    Recall {i}:  train = {tr:.2f}, valid = {vr:.2f}')\n",
    "        print(f'    F1 {i}:  train = {tf:.2f}, valid = {vf:.2f}')\n",
    "        writer.add_scalars('precision', {f'train {i}': tp, f'valid {i}': vp}, epoch)\n",
    "        writer.add_scalars('recall', {f'train {i}': tr, f'valid {i}': vr}, epoch)\n",
    "        writer.add_scalars('f1', {f'train {i}': tf, f'valid {i}': vf}, epoch)\n",
    "\n",
    "    mean_train_precision = tps.mean()\n",
    "    mean_valid_precision = vps.mean()\n",
    "    mean_train_recall = trs.mean()\n",
    "    mean_valid_recall = vrs.mean()\n",
    "    mean_train_f1 = tfs.mean()\n",
    "    mean_valid_f1 = vfs.mean()\n",
    "\n",
    "    print(f'    Precision:  train = {mean_train_precision:.2f}, valid = {mean_valid_precision:.2f}')\n",
    "    print(f'    Recall:  train = {mean_train_recall:.2f}, valid = {mean_valid_recall:.2f}')\n",
    "    print(f'    F1:  train = {mean_train_f1:.2f}, valid = {mean_valid_f1:.2f}')\n",
    "    writer.add_scalars('precision', {f'train': mean_train_precision, f'valid': mean_valid_precision}, epoch)\n",
    "    writer.add_scalars('recall', {f'train': mean_train_recall, f'valid': mean_valid_recall}, epoch)\n",
    "    writer.add_scalars('f1', {f'train': mean_train_f1, f'valid': mean_valid_f1}, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Test\n",
    "\n",
    "## 4.1 Define test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        'ent': 1000,\n",
    "        'classes': [1, 0, 1, 0],  # married, male, American, actor\n",
    "        'sents': [\n",
    "            'Michelle is married',\n",
    "            'Michelle is female',\n",
    "            'Michelle is American'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'ent': 2000,\n",
    "        'classes': [1, 0, 0, 0],  # married, male, American, actor\n",
    "        'sents': [\n",
    "            'Angela is married',\n",
    "            'Angela is female',\n",
    "            'Angela is German'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "test_set = [Sample(\n",
    "    item['ent'],\n",
    "    item['classes'],\n",
    "    [[vocab[word] for word in tokenize(sent)] for sent in item['sents']]\n",
    ") for item in test_data]\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set), collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Forward test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for sents_batch, classes_batch in test_loader:\n",
    "        logits_batch = classifier(sents_batch)\n",
    "\n",
    "        print(logits_batch)\n",
    "\n",
    "        loss = criterion(logits_batch, classes_batch.float())\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "print(f'Test loss = {test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}