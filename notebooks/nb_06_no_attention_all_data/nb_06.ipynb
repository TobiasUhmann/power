{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feb 25\n",
    "\n",
    "For some reason the valid loss does not decrease significantly\n",
    "on the attention model. Compare with a baseline that does not\n",
    "include the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from IPython.lib.pretty import pretty\n",
    "from torch import tensor, Tensor\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data import Field, TabularDataset\n",
    "\n",
    "from classifier import Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Build train/valid DataLoaders\n",
    "\n",
    "## 1.1 Read Sample TSVs into TabularDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tobias\\documents\\hsrm\\master\\thesis\\thesis-tools\\venv\\lib\\site-packages\\torchtext\\data\\field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "c:\\users\\tobias\\documents\\hsrm\\master\\thesis\\thesis-tools\\venv\\lib\\site-packages\\torchtext\\data\\example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "c:\\users\\tobias\\documents\\hsrm\\master\\thesis\\thesis-tools\\venv\\lib\\site-packages\\torchtext\\data\\example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity 1\n",
      "classes: married = 0, male = 0, American = 0, actor = 0\n",
      "['the', 'belarusian', \"people's\", 'republic', 'was', 'declared', 'on', 'the', 'german-occupied', 'territory', 'of', 'modern-day', 'belarus', 'three', 'weeks', 'after', 'the', 'treaty', 'of', 'brest-litovsk', 'was', 'signed', 'on', 'march', '3,', '1918', 'between', 'the', 'new', 'bolshevik', 'government', 'of', 'soviet', 'russia', 'and', 'the', 'central', 'powers', 'in', 'the', 'border', 'city', 'of', 'brest-litovsk.']\n",
      "['some', 'of', 'the', 'action', 'in', 'joe', \"haldeman's\", '1989', 'science', 'fiction', 'novel', 'buying', 'time', '(published', 'in', 'uk', 'as', 'the', 'long', 'habit', 'of', 'living)', 'takes', 'place', 'in', 'the', 'conch', 'republic,', 'a', 'lawless', 'place', 'where', 'assassination', 'and', 'other', 'activities', 'are', 'perfectly', 'legal.']\n",
      "['the', 'republic', 'of', 'minerva', 'was', 'a', 'micronation', 'consisting', 'of', 'the', 'minerva', 'reefs.']\n",
      "\n",
      "entity 2\n",
      "classes: married = 0, male = 0, American = 0, actor = 0\n",
      "['at', 'age', '14,', 'he', 'appeared', 'in', 'the', 'series', 'mighty', 'morphin', 'power', 'rangers', 'as', 'an', 'uncredited', 'extra', 'in', 'the', 'episode', '\"second', 'chance\".']\n",
      "['in', 'the', 'mighty', 'morphin', 'alien', 'rangers', 'mini', 'series,', 'which', 'sets', 'up', 'the', 'transition', 'from', 'mighty', 'morphin', 'power', 'rangers', 'to', 'power', 'rangers', 'zeo,', 'goldar', 'and', 'rito', 'plants', 'a', 'bomb', 'outside', 'of', 'the', 'command', 'center,', 'but', 'it', 'is', 'defused', 'by', 'alpha', '5.']\n",
      "['mighty', 'morphin', 'power', 'rangers', 'has', 'gained', 'mostly', 'positive', 'reviews,', 'with', 'the', '#1', 'issue', 'selling', 'approximately', '100,000', 'copies.']\n",
      "\n",
      "entity 4\n",
      "classes: married = 0, male = 0, American = 0, actor = 0\n",
      "['a', 'free', 'soul', 'is', 'a', '1931', 'american', 'pre-code', 'drama', 'film', 'that', 'tells', 'the', 'story', 'of', 'an', 'alcoholic', 'san', 'francisco', 'defense', 'attorney', 'who', 'must', 'defend', 'his', \"daughter's\", 'ex-boyfriend', 'on', 'a', 'charge', 'of', 'murdering', 'the', 'mobster', 'she', 'had', 'started', 'a', 'relationship', 'with,', 'who', 'he', 'had', 'previously', 'gotten', 'an', 'acquittal', 'for', 'on', 'a', 'murder', 'charge.']\n",
      "['itzkoff', 'wrote', 'that', 'critics', 'fear', 'that', '\"rape', 'has', 'become', 'so', 'pervasive', 'in', 'the', 'drama', 'that', 'it', 'is', 'almost', 'background', 'noise:', 'a', 'routine', 'and', 'unshocking', 'occurrence\".']\n",
      "['gavagai', 'is', 'a', '2016', 'norwegian', 'drama', 'film', 'directed', 'by', 'rob', 'tregenza.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_count = 4\n",
    "sent_count = 3\n",
    "\n",
    "#\n",
    "# Define columns for subsequent read into TabularDatasets\n",
    "#\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return text.split()\n",
    "\n",
    "ent_field = ('ent', Field(sequential=False, use_vocab=False))\n",
    "\n",
    "class_fields = [(f'class_{i}', Field(sequential=False, use_vocab=False))\n",
    "                for i in range(class_count)]\n",
    "\n",
    "sent_fields = [(f'sent_{i}', Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True))\n",
    "               for i in range(sent_count)]\n",
    "\n",
    "fields = [ent_field] + class_fields + sent_fields\n",
    "\n",
    "#\n",
    "# Read Train Samples TSV into TabularDataset\n",
    "#\n",
    "\n",
    "train_samples_tsv = 'data/ower-v3-fb-3/train.tsv'\n",
    "valid_samples_tsv = 'data/ower-v3-fb-3/valid.tsv'\n",
    "# test_samples_tsv = 'data/ower-v3-fb-3/test.tsv'\n",
    "\n",
    "raw_train_set = TabularDataset(train_samples_tsv, 'tsv', fields, skip_header=True)\n",
    "raw_valid_set = TabularDataset(valid_samples_tsv, 'tsv', fields, skip_header=True)\n",
    "# raw_test_set = TabularDataset(test_samples_tsv, 'tsv', fields, skip_header=True)\n",
    "\n",
    "for i in range(3):\n",
    "    row = raw_train_set[i]\n",
    "\n",
    "    print('entity', row.ent)\n",
    "    print('classes: married = {}, male = {}, American = {}, actor = {}'.format(\n",
    "        row.class_0, row.class_1, row.class_2, row.class_3))\n",
    "    print(row.sent_0)\n",
    "    print(row.sent_1)\n",
    "    print(row.sent_2)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2 Build vocab on train data\n",
    "\n",
    "The docs only show how to build a vocab over a single column. Therefore, the vocab\n",
    "is built over the first sentence column only atm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55674\n",
      "['<unk>', '<pad>', 'the', 'and', 'in', 'of', 'a', 'to', 'for', 'was']\n",
      "['cfas', 'cfc', 'cfr', 'cgbd', 'cgi', 'cgi-animated', 'chace', 'chacha', 'chachi', 'chad,']\n",
      "['€6', '明', '曹轩领4内援2外援加盟', '曾三度临危受命', '艳光四射)', '艷光四射,', '관광체험),', '및', '배밭', '조형익;']\n"
     ]
    }
   ],
   "source": [
    "first_sent_field = sent_fields[0][1]\n",
    "first_sent_field.build_vocab(raw_train_set)\n",
    "vocab = first_sent_field.vocab\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab_size)\n",
    "print(vocab.itos[:10])\n",
    "print(vocab.itos[vocab_size//2:vocab_size//2+10])\n",
    "print(vocab.itos[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.3 Transfor each TabularDataset -> List[Sample]\n",
    "\n",
    "Parse texts from datasets, map words -> tokens (IDs) using vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training sample:\n",
      "Sample(ent=1, classes=[0, 0, 0, 0], sents=[[2, 7023, 1753, 958, 9, 1911, 12, 2, 34261, 2789, 5, 16205, 8971, 81, 1434, 32, 2, 2796, 5, 12945, 9, 218, 12, 143, 494, 12042, 112, 2, 27, 9024, 499, 5, 1665, 2086, 3, 2, 321, 3252, 4, 2, 1121, 88, 5, 26693], [150, 5, 2, 423, 4, 293, 0, 1601, 290, 538, 299, 13031, 106, 0, 4, 825, 10, 2, 485, 35190, 5, 0, 1109, 244, 4, 2, 0, 6509, 6, 39251, 244, 60, 8877, 3, 57, 3732, 44, 10729, 0], [2, 958, 5, 16163, 9, 6, 0, 2013, 5, 2, 16163, 0]])\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Sample:\n",
    "    ent: int\n",
    "    classes: List[int]\n",
    "    sents: List[List[int]]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter((self.ent, self.classes, self.sents))\n",
    "\n",
    "\n",
    "def transform(raw_set: TabularDataset) -> List[Sample]:\n",
    "    return [Sample(\n",
    "        int(getattr(row, 'ent')),\n",
    "        [int(getattr(row, f'class_{i}')) for i in range(class_count)],\n",
    "        [[vocab[token] for token in getattr(row, f'sent_{i}')] for i in range(sent_count)]\n",
    "    ) for row in raw_set]\n",
    "\n",
    "\n",
    "train_set = transform(raw_train_set)\n",
    "valid_set = transform(raw_valid_set)\n",
    "# test_set = transform(raw_test_set)\n",
    "\n",
    "print('First training sample:')\n",
    "print(pretty(train_set[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.4 Build DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch: List[Sample]) -> Tuple[Tensor, Tensor]:\n",
    "\n",
    "    _ent, classes_batch, sents_batch = zip(*batch)\n",
    "\n",
    "    cropped_sents_batch = [[sent[:sent_len]\n",
    "                            for sent in sents] for sents in sents_batch]\n",
    "\n",
    "    padded_sents_batch = [[sent + [0] * (sent_len - len(sent))\n",
    "                           for sent in sents] for sents in cropped_sents_batch]\n",
    "\n",
    "    return tensor(padded_sents_batch), tensor(classes_batch)\n",
    "\n",
    "batch_size = 1024\n",
    "sent_len = 64\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=generate_batch, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, collate_fn=generate_batch)\n",
    "# test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2 Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Classifier(\n  (embedding_bag): EmbeddingBag(55674, 256, mode=mean)\n  (linear): Linear(in_features=256, out_features=4, bias=True)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_size = 256\n",
    "\n",
    "classifier = Classifier(vocab_size, emb_size, class_count)\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss = 6.0900243918101, valid loss = 4.787463188171387\n",
      "Epoch 1: Train loss = 2.0882804095745087, valid loss = 2.864480972290039\n",
      "Epoch 2: Train loss = 1.096674417455991, valid loss = 2.0682241916656494\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-9-b12d43c067b1>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     24\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 26\u001B[1;33m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m     \u001B[0mtrain_loss\u001B[0m \u001B[1;33m/=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tobias\\documents\\hsrm\\master\\thesis\\thesis-tools\\venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     24\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 26\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mF\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tobias\\documents\\hsrm\\master\\thesis\\thesis-tools\\venv\\lib\\site-packages\\torch\\optim\\adam.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m             \u001B[0mbeta1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbeta2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgroup\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'betas'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m             F.adam(params_with_grad,\n\u001B[0m\u001B[0;32m    109\u001B[0m                    \u001B[0mgrads\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m                    \u001B[0mexp_avgs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\tobias\\documents\\hsrm\\master\\thesis\\thesis-tools\\venv\\lib\\site-packages\\torch\\optim\\functional.py\u001B[0m in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001B[0m\n\u001B[0;32m     92\u001B[0m             \u001B[0mdenom\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mmax_exp_avg_sq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbias_correction2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0meps\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     93\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 94\u001B[1;33m             \u001B[0mdenom\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mexp_avg_sq\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mmath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbias_correction2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0meps\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     95\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     96\u001B[0m         \u001B[0mstep_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlr\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mbias_correction1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# criterion = MSELoss()\n",
    "# criterion = BCEWithLogitsLoss()\n",
    "criterion = BCEWithLogitsLoss(pos_weight=torch.tensor([80] * class_count))\n",
    "\n",
    "# optimizer = SGD(classifier.parameters(), lr=0.1)\n",
    "optimizer = Adam(classifier.parameters(), lr=0.1)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "\n",
    "    #\n",
    "    # Train\n",
    "    #\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for sents_batch, classes_batch in tqdm(train_loader, leave=False):\n",
    "        logits_batch = classifier(sents_batch)\n",
    "\n",
    "        loss = criterion(logits_batch, classes_batch.float())\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    #\n",
    "    # Validate\n",
    "    #\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for sents_batch, classes_batch in tqdm(valid_loader, leave=False):\n",
    "            logits_batch = classifier(sents_batch)\n",
    "\n",
    "            loss = criterion(logits_batch, classes_batch.float())\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    #\n",
    "    # Log\n",
    "    #\n",
    "\n",
    "    print(f'Epoch {epoch}: Train loss = {train_loss}, valid loss = {valid_loss}')\n",
    "    \n",
    "    writer.add_scalars('loss', {'train': train_loss, 'valid': valid_loss}, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4 Test\n",
    "\n",
    "## 4.1 Define test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        'ent': 1000,\n",
    "        'classes': [1, 0, 1, 0],  # married, male, American, actor\n",
    "        'sents': [\n",
    "            'Michelle is married',\n",
    "            'Michelle is female',\n",
    "            'Michelle is American'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'ent': 2000,\n",
    "        'classes': [1, 0, 0, 0],  # married, male, American, actor\n",
    "        'sents': [\n",
    "            'Angela is married',\n",
    "            'Angela is female',\n",
    "            'Angela is German'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "test_set = [Sample(\n",
    "    item['ent'],\n",
    "    item['classes'],\n",
    "    [[vocab[word] for word in tokenize(sent)] for sent in item['sents']]\n",
    ") for item in test_data]\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set), collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Forward test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for sents_batch, classes_batch in test_loader:\n",
    "        logits_batch = classifier(sents_batch)\n",
    "        \n",
    "        print(logits_batch)\n",
    "\n",
    "        loss = criterion(logits_batch, classes_batch.float())\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "print(f'Test loss = {test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}