{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feb 21\n",
    "\n",
    "The OWER classifier does learn something - training loss decreases\n",
    " as does validation until the 5th epoch or so - but\n",
    "validation loss decreases only slightly, and the results on\n",
    "\"Barack Obama is a married, male, American actor\" look bad.\n",
    "\n",
    "It is worth turning the weights of the loss function but still the\n",
    "resulting classes for Obama differ randomly from run to run. Also,\n",
    "plotting the class/word attentions shows that those converge, but\n",
    "the observable correlations are not as expected.\n",
    "\n",
    "In the following a minimal example shall be constructed to proof\n",
    "the concept behind the classifier. An entity with a few short\n",
    "sentences from a small vocabulary shall be forwarded through a\n",
    "pre-trained OWER classifier whose class embeddings perfectly match\n",
    "the sentences so that the correct classes should be predicted.\n",
    "\n",
    "Example scenario:\n",
    "\n",
    "```\n",
    "Input sentences: \"married married\", \"male male\", \"american american\"\n",
    "Expected classes: married=True, male=True, american=True, actor=?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.lib.pretty import pretty\n",
    "\n",
    "def log_tensor(tensor, title, labels, vmin=None, vmax=None):\n",
    "    pass\n",
    "\n",
    "%run util.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1 Input data\n",
    "\n",
    "Given an input such as `raw_input` in the following, pre-processing\n",
    "should yield a `sents_batch` and a `classes_batch`, each of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    { 'ent': 123, 'classes': [1, 1, 1], 'sents': ['married married married', 'male male male', 'American American American'] },\n",
    "    { 'ent': 123, 'classes': [1, 1, 1], 'sents': ['married married married', 'male male male', 'American American American'] },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2 Pre-processing\n",
    "\n",
    "## 2.1 Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "words = [word for ent in data for sent in ent['sents'] for word in tokenize(sent)]\n",
    "vocab = Vocab(Counter(words))\n",
    "\n",
    "print(pretty(vocab.stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2 Transform data\n",
    "\n",
    "Map words to tokens and create tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "\n",
    "sents_batch = tensor([[[vocab[word] for word in tokenize(sent)] for sent in ent['sents']] for ent in data])\n",
    "classes_batch = torch.tensor([ent['classes'] for ent in data])\n",
    "\n",
    "assert len(sents_batch) == len(classes_batch)\n",
    "\n",
    "batch_size = len(sents_batch)\n",
    "sent_count = 3\n",
    "sent_len = 3\n",
    "\n",
    "ent_labels = [f'ent {i}' for i in range(batch_size)]\n",
    "sent_labels = [f'sent {i}' for i in range(sent_count)]\n",
    "tok_labels = [f'tok {i}' for i in range(sent_len)]\n",
    "\n",
    "log_tensor(sents_batch, 'sents_batch', [ent_labels, sent_labels, tok_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3 Prepare classifier\n",
    "\n",
    "## 3.1 Prepare EmbeddingBag\n",
    "\n",
    "Create and prepare an `EmbeddingBag` so that each token has a unique,\n",
    "easily memorable embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import EmbeddingBag\n",
    "from torch import tensor\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "assert vocab_size == 5\n",
    "\n",
    "emb_size = 4\n",
    "\n",
    "embedding_bag = EmbeddingBag(num_embeddings=vocab_size, embedding_dim=emb_size)\n",
    "\n",
    "embedding_bag.weight.data = tensor([\n",
    "    [0., 0., 0., 0.],\n",
    "    [1., 0., 0., 0.],\n",
    "    [0., 1., 0., 0.],\n",
    "    [0., 0., 1., 0.],\n",
    "    [0., 0., 0., 1.]\n",
    "])\n",
    "\n",
    "old_embedding_bag_weight_data = embedding_bag.weight.data.detach().clone()\n",
    "\n",
    "log_output = 1\n",
    "\n",
    "word_labels = ['<unk>', '<pad>', 'married', 'male', 'American']\n",
    "emb_labels = [f'emb {i}' for i in range(emb_size)]\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(embedding_bag.weight, 'embedding_bag.weight', [word_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2 Prepare class embeddings\n",
    "\n",
    "Create and prepare the class embeddings so that there is one class embedding\n",
    "per non-special token that perfectly matches that token's embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_embs = tensor([\n",
    "    [0., 1., 0., 0.],\n",
    "    [0., 0., 1., 0.],\n",
    "    [0., 0., 0., 1.]\n",
    "], requires_grad=True)\n",
    "\n",
    "old_class_embs = class_embs.detach().clone()\n",
    "\n",
    "log_output = 1\n",
    "\n",
    "class_labels = ['married', 'male', 'American']\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(class_embs, 'class_embs', [class_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.3 Prepare linear layer\n",
    "\n",
    "Create and prepare the linear layer so that it completely relies on\n",
    "the first class' sentence mix to predict the first class, the second\n",
    "class' sentence mix to predict the second class, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "class_count = len(class_embs)\n",
    "assert class_count == 3\n",
    "\n",
    "linear = Linear(class_count * emb_size, class_count)\n",
    "\n",
    "linear.weight.data = tensor([\n",
    "    [1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.]\n",
    "])\n",
    "\n",
    "linear.bias.data = tensor([0., 0., 0.])\n",
    "\n",
    "old_linear_weight_data = linear.weight.data.detach().clone()\n",
    "old_linear_bias_data = linear.bias.data.detach().clone()\n",
    "\n",
    "log_output = 1\n",
    "\n",
    "mix_emb_labels = [f'mix {i} / emb {j}' for i in range(class_count) for j in range(emb_size)]\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(linear.weight.data, 'linear.weight.data', [class_labels, mix_emb_labels])\n",
    "    log_tensor(linear.bias.data, 'linear.bias.data', [class_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4 Forward\n",
    "\n",
    "## 4.1 Embed sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Flatten batch\n",
    "#\n",
    "# < sents_batch  (batch_size, sent_count, sent_len)\n",
    "# > flat_sents   (batch_size * sent_count, sent_len)\n",
    "#\n",
    "\n",
    "flat_sents = sents_batch.reshape(batch_size * sent_count, sent_len)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "ent_sent_labels = [f'ent {i} / sent {j}' for i in range(batch_size) for j in range(sent_count)]\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(sents_batch, 'sents_batch', [ent_labels, sent_labels, tok_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(flat_sents, 'flat_sents', [ent_sent_labels, tok_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Embed sentences\n",
    "#\n",
    "# < embedding_bag.weight  (vocab_size, emb_size)\n",
    "# < flat_sents            (batch_size * sent_count, sent_len)\n",
    "# > flat_sent_embs        (batch_size * sent_count, emb_size)\n",
    "#\n",
    "\n",
    "flat_sent_embs = embedding_bag(flat_sents)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(embedding_bag.weight, 'embedding_bag.weight', [word_labels, emb_labels])\n",
    "    log_tensor(flat_sents, 'flat_sents', [ent_sent_labels, tok_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(flat_sent_embs, 'flat_sent_embs', [ent_sent_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Restore batch\n",
    "#\n",
    "# < flat_sent_embs   (batch_size * sent_count, emb_size)\n",
    "# > sent_embs_batch  (batch_size, sent_count, emb_size)\n",
    "#\n",
    "\n",
    "sent_embs_batch = flat_sent_embs.reshape(batch_size, sent_count, emb_size)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 1\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(flat_sent_embs, 'flat_sent_embs', [ent_sent_labels, emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(sent_embs_batch, 'sent_embs_batch', [ent_labels, sent_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.2 Calc attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Expand class embeddings for bmm()\n",
    "#\n",
    "# < class_embs        (class_count, emb_size)\n",
    "# > class_embs_batch  (batch_size, class_count, emb_size)\n",
    "#\n",
    "\n",
    "class_embs_batch = class_embs.expand(batch_size, class_count, emb_size)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(class_embs, 'class_embs', [class_labels, emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(class_embs_batch, 'class_embs_batch', [ent_labels, class_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Multiply each class with each sentence\n",
    "#\n",
    "# < class_embs_batch    (batch_size, class_count, emb_size)\n",
    "# < sent_embs_batch     (batch_size, sent_count, emb_size)\n",
    "# > atts_batch          (batch_size, class_count, sent_count)\n",
    "#\n",
    "\n",
    "atts_batch = torch.bmm(class_embs_batch, sent_embs_batch.transpose(1, 2))\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(class_embs_batch, 'class_embs_batch', [ent_labels, class_labels, emb_labels])\n",
    "    log_tensor(sent_embs_batch, 'sent_embs_batch', [ent_labels, sent_labels, emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(atts_batch, 'atts_batch', [ent_labels, class_labels, sent_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Apply softmax over sentences\n",
    "#\n",
    "# < atts_batch      (batch_size, class_count, sent_count)\n",
    "# > softs_batch     (batch_size, class_count, sent_count)\n",
    "#\n",
    "\n",
    "from torch.nn import Softmax\n",
    "\n",
    "softs_batch = Softmax(dim=-1)(atts_batch)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 1\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(atts_batch, 'atts_batch', [ent_labels, class_labels, sent_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(softs_batch, 'softs_batch', [ent_labels, class_labels, sent_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Weight and mix sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Repeat each batch slice class_count times\n",
    "#\n",
    "# < sent_embs_batch     (batch_size, sent_count, emb_size)\n",
    "# > expaned_batch       (batch_size, class_count, sent_count, emb_size)\n",
    "#\n",
    "\n",
    "expaned_batch = sent_embs_batch.unsqueeze(1).expand(-1, class_count, -1, -1)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(sent_embs_batch, 'sent_embs_batch', [ent_labels, sent_labels, emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(expaned_batch, 'expaned_batch', [ent_labels, class_labels, sent_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Flatten sentences for bmm()\n",
    "#\n",
    "# < expaned_batch   (batch_size, class_count, sent_count, emb_size)\n",
    "# > flat_expanded   (batch_size * class_count, sent_count, emb_size)\n",
    "#\n",
    "\n",
    "flat_expanded = expaned_batch.reshape(-1, sent_count, emb_size)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "ent_class_labels = [f'ent {i} / class {j}' for i in range(batch_size) for j in range(class_count)]\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(expaned_batch, 'expaned_batch', [ent_labels, class_labels, sent_labels, emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(flat_expanded, 'flat_expanded', [ent_class_labels, sent_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Flatten attentions for bmm()\n",
    "#\n",
    "# < softs_batch     (batch_size, class_count, sent_count)\n",
    "# > flat_softs      (batch_size * class_count, sent_count, 1)\n",
    "#\n",
    "\n",
    "flat_softs = softs_batch.reshape(batch_size * class_count, sent_count).unsqueeze(-1)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(softs_batch, 'softs_batch', [ent_labels, class_labels, sent_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(flat_softs, 'flat_softs', [ent_class_labels, sent_labels, ['']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Multiply each sentence with each attention\n",
    "#\n",
    "# < flat_expanded   (batch_size * class_count, sent_count, emb_size)\n",
    "# < flat_softs      (batch_size * class_count, sent_count, 1)\n",
    "# > flat_weighted   (batch_size * class_count, emb_size)\n",
    "#\n",
    "\n",
    "flat_weighted = torch.bmm(flat_expanded.transpose(1, 2), flat_softs).squeeze(-1)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 0\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(flat_expanded, 'flat_expanded', [ent_class_labels, sent_labels, emb_labels])\n",
    "    log_tensor(flat_softs, 'flat_softs', [ent_class_labels, sent_labels, ['']])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(flat_weighted, 'flat_weighted', [ent_class_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Restore batch\n",
    "#\n",
    "# < flat_weighted   (batch_size * class_count, emb_size)\n",
    "# > weighted_batch  (batch_size, class_count, emb_size)\n",
    "#\n",
    "\n",
    "weighted_batch = flat_weighted.reshape(batch_size, class_count, emb_size)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 1\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(flat_weighted, 'flat_weighted', [ent_class_labels, emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(weighted_batch, 'weighted_batch', [ent_labels, class_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.4 Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Concatenate mixes\n",
    "#\n",
    "# < weighted_batch  (batch_size, class_count, emb_size)\n",
    "# > concat_mixes_batch  (batch_size, class_count * emb_size)\n",
    "#\n",
    "\n",
    "concat_mixes_batch = weighted_batch.reshape(batch_size, class_count * emb_size)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 1\n",
    "\n",
    "mix_emb_labels = [f'mix {i} / emb {j}' for i in range(class_count) for j in range(emb_size)]\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(weighted_batch, 'weighted_batch', [ent_labels, class_labels, emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(concat_mixes_batch, 'concat_mixes_batch', [ent_labels, mix_emb_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Push concatenated mixes through linear layer\n",
    "#\n",
    "# < concat_mixes_batch  (batch_size, class_count * emb_size)\n",
    "# > logits_batch        (batch_size, class_count)\n",
    "#\n",
    "\n",
    "logits_batch = linear(concat_mixes_batch)\n",
    "\n",
    "log_input = 0\n",
    "log_output = 1\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(concat_mixes_batch, 'concat_mixes_batch', [ent_labels, mix_emb_labels])\n",
    "\n",
    "if log_output:\n",
    "    log_tensor(logits_batch, 'logits_batch', [ent_labels, class_labels], vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4.5 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Push concatenated mixes through linear layer\n",
    "#\n",
    "# < concat_mixes_batch  (batch_size, class_count * emb_size)\n",
    "# > logits_batch        (batch_size, class_count)\n",
    "#\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "criterion = BCEWithLogitsLoss(pos_weight=torch.tensor([80] * class_count))\n",
    "# criterion = torch.nn.MSELoss()\n",
    "\n",
    "loss = criterion(logits_batch, classes_batch.float())\n",
    "\n",
    "log_input = 0\n",
    "log_output = 1\n",
    "\n",
    "if log_input:\n",
    "    log_tensor(logits_batch, 'logits_batch', [ent_labels, class_labels], vmin=-1, vmax=1)\n",
    "    log_tensor(classes_batch, 'classes_batch', [ent_labels, class_labels], vmin=-1, vmax=1)\n",
    "\n",
    "log_tensor(loss, 'loss', [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5 Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam([embedding_bag.weight, class_embs, linear.weight, linear.bias], lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer.step()\n",
    "\n",
    "vmin = min(torch.min(old_embedding_bag_weight_data), torch.min(embedding_bag.weight.data))\n",
    "vmax = max(torch.max(old_embedding_bag_weight_data), torch.max(embedding_bag.weight.data))\n",
    "\n",
    "log_tensor(old_embedding_bag_weight_data, 'old_embedding_bag_weight_data', [word_labels, emb_labels], vmin=vmin, vmax=vmax)\n",
    "log_tensor(embedding_bag.weight.data.detach(), 'embedding_bag.weight.data', [word_labels, emb_labels], vmin=vmin, vmax=vmax)\n",
    "\n",
    "vmin = min(torch.min(old_class_embs), torch.min(class_embs))\n",
    "vmax = max(torch.max(old_class_embs), torch.max(class_embs))\n",
    "\n",
    "log_tensor(old_class_embs, 'old_class_embs', [class_labels, emb_labels], vmin=vmin, vmax=vmax)\n",
    "log_tensor(class_embs.detach(), 'class_embs', [class_labels, emb_labels], vmin=vmin, vmax=vmax)\n",
    "\n",
    "vmin = min(torch.min(old_linear_weight_data), torch.min(linear.weight.data))\n",
    "vmax = max(torch.max(old_linear_weight_data), torch.max(linear.weight.data))\n",
    "\n",
    "log_tensor(old_linear_weight_data, 'old_linear_weight_data', [class_labels, mix_emb_labels], vmin=vmin, vmax=vmax)\n",
    "log_tensor(linear.weight.data.detach(), 'linear.weight.data', [class_labels, mix_emb_labels], vmin=vmin, vmax=vmax)\n",
    "\n",
    "vmin = min(torch.min(old_linear_bias_data), torch.min(linear.bias.data))\n",
    "vmax = max(torch.max(old_linear_bias_data), torch.max(linear.bias.data))\n",
    "\n",
    "log_tensor(old_linear_bias_data, 'old_linear_weight_data', [class_labels], vmin=vmin, vmax=vmax)\n",
    "log_tensor(linear.bias.data.detach(), 'linear.bias.data', [class_labels], vmin=vmin, vmax=vmax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}