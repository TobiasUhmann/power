{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feb 24\n",
    "\n",
    "The training in notebook _03_overfit_and_test seems to work. The classifier\n",
    "overfits on the training data and correctly predicts when given the training\n",
    "data for testing.\n",
    "\n",
    "Now, the classifier code should be enhanced to a PyTorch module that is trained\n",
    "in a training + validation loop as it is the case in the code base. The loss\n",
    "curve should be printed as well.\n",
    "\n",
    "The expected result is a loss curve similar to the one resulting from the code\n",
    "base, i.e. validation loss should drop until around the 5th epoch after which\n",
    "it should rise again\n",
    "\n",
    "Hopefully, this notebook provides the means to debug and understand why the\n",
    "validation loss curve does not drop further and thus, why the classifier does\n",
    "not yield better test results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from IPython.lib.pretty import pretty\n",
    "from torch import tensor\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "from notebooks._04_classifier import util\n",
    "from notebooks._04_classifier.classifier import Classifier\n",
    "from notebooks._04_classifier.util import log_tensor, get_ent_lbls, get_sent_lbls, get_tok_lbls, get_emb_lbls, \\\n",
    "    get_class_lbls, get_mix_emb_lbls, get_word_lbls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        'classes': [1, 1, 1],\n",
    "        'sents': [\n",
    "            'married married married',\n",
    "            'male male male',\n",
    "            'American American American'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'classes': [0, 0, 0],\n",
    "        'sents': [\n",
    "            'single single single',\n",
    "            'female female female',\n",
    "            'German German German'\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "batch_size = 2\n",
    "class_count = 3\n",
    "emb_size = 4\n",
    "sent_count = 3\n",
    "sent_len = 3\n",
    "\n",
    "util.batch_size = batch_size\n",
    "util.class_count = class_count\n",
    "util.emb_size = emb_size\n",
    "util.sent_count = sent_count\n",
    "util.sent_len = sent_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Pre-processing\n",
    "\n",
    "## 2.1 Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "words = [word for ent in data for sent in ent['sents'] for word in tokenize(sent)]\n",
    "vocab = Vocab(Counter(words))\n",
    "\n",
    "print(pretty(vocab.stoi))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "util.vocab_size = vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transform data\n",
    "\n",
    "Map words to tokens and create tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sents_batch = tensor([[[vocab[word] for word in tokenize(sent)] for sent in ent['sents']] for ent in data])\n",
    "classes_batch = torch.tensor([ent['classes'] for ent in data])\n",
    "assert len(sents_batch) == len(classes_batch)\n",
    "\n",
    "log_tensor(sents_batch, 'sents_batch', [get_ent_lbls(), get_sent_lbls(), get_tok_lbls()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3 Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classifier = Classifier(vocab_size, emb_size, class_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Forward & Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "criterion = BCEWithLogitsLoss(pos_weight=torch.tensor([80] * class_count))\n",
    "# criterion = MSELoss()\n",
    "\n",
    "optimizer = Adam(classifier.parameters(), lr=0.1)\n",
    "\n",
    "epoch_count = 1001\n",
    "for epoch in range(epoch_count):\n",
    "\n",
    "    if epoch in [0, 10, 100, 1000]:\n",
    "        print(epoch)\n",
    "\n",
    "        # log_tensor(classifier.embedding_bag.weight.detach(), 'classifier.embedding_bag.weight', [get_word_lbls(), get_emb_lbls()])\n",
    "        # log_tensor(classifier.class_embs.detach(), 'classifier.class_embs', [get_class_lbls(), get_emb_lbls()])\n",
    "        # log_tensor(classifier.linear.weight.data, 'classifier.linear.weight.data', [get_class_lbls(), get_mix_emb_lbls()])\n",
    "        # log_tensor(classifier.linear.bias.data, 'classifier.linear.bias.data', [get_class_lbls()])\n",
    "\n",
    "    logits_batch = classifier(sents_batch)\n",
    "\n",
    "    #\n",
    "    # Loss\n",
    "    #\n",
    "\n",
    "    loss = criterion(logits_batch, classes_batch.float())\n",
    "    print(loss.item())\n",
    "\n",
    "    # log_tensor(logits_batch, 'logits_batch', [get_ent_lbls(), get_class_lbls()])\n",
    "    # log_tensor(classes_batch, 'classes_batch', [get_ent_lbls(), get_class_lbls()])\n",
    "    # log_tensor(loss, 'loss', [])\n",
    "\n",
    "    #\n",
    "    # Backward\n",
    "    #\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5 Test\n",
    "\n",
    "## 5.1 Define test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test_data = [\n",
    "#     {\n",
    "#         'classes': [1, 1, 1],\n",
    "#         'sents': [\n",
    "#             'married married married',\n",
    "#             'male male male',\n",
    "#             'American American American'\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         'classes': [0, 0, 0],\n",
    "#         'sents': [\n",
    "#             'single single single',\n",
    "#             'female female female',\n",
    "#             'German German German'\n",
    "#         ]\n",
    "#     },\n",
    "# ]\n",
    "#\n",
    "# batch_size = 2\n",
    "# class_count = 3\n",
    "# emb_size = 4\n",
    "# sent_count = 3\n",
    "# sent_len = 3\n",
    "#\n",
    "# util.batch_size = batch_size\n",
    "# util.class_count = class_count\n",
    "# util.emb_size = emb_size\n",
    "# util.sent_count = sent_count\n",
    "# util.sent_len = sent_len\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        'classes': [1, 1, 1],  # married, male, American\n",
    "        'sents': [\n",
    "            'Barack Obama is married',\n",
    "            'Barack Obama is male',\n",
    "            'Barack Obama is American'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'classes': [1, 0, 1],  # married, male, American\n",
    "        'sents': [\n",
    "            'Michelle Obama is married',\n",
    "            'Michelle Obama is female',\n",
    "            'Michelle Obama is American'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'classes': [1, 0, 0],  # married, male, American\n",
    "        'sents': [\n",
    "            'Angela Merkel is married',\n",
    "            'Angela Merkel is female',\n",
    "            'Angela Merkel is German'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "batch_size = 3\n",
    "class_count = 3\n",
    "emb_size = 4\n",
    "sent_count = 3\n",
    "sent_len = 4\n",
    "\n",
    "util.batch_size = batch_size\n",
    "util.class_count = class_count\n",
    "util.emb_size = emb_size\n",
    "util.sent_count = sent_count\n",
    "util.sent_len = sent_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Pre-process test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_sents_batch = tensor([[[vocab[word] for word in tokenize(sent)] for sent in ent['sents']] for ent in test_data])\n",
    "test_classes_batch = torch.tensor([ent['classes'] for ent in test_data])\n",
    "assert len(sents_batch) == len(classes_batch)\n",
    "\n",
    "log_tensor(test_sents_batch, 'test_sents_batch', [get_ent_lbls(), get_sent_lbls(), get_tok_lbls()])\n",
    "log_tensor(test_classes_batch, 'test_classes_batch', [get_ent_lbls(), get_class_lbls()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Forward test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_logits_batch = classifier(test_sents_batch)\n",
    "\n",
    "#\n",
    "# Loss\n",
    "#\n",
    "\n",
    "test_loss = criterion(test_logits_batch, test_classes_batch.float())\n",
    "\n",
    "log_tensor(test_logits_batch, 'test_logits_batch', [get_ent_lbls(), get_class_lbls()])\n",
    "log_tensor(test_classes_batch, 'test_classes_batch', [get_ent_lbls(), get_class_lbls()])\n",
    "log_tensor(test_loss, 'test_loss', [])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}