{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feb 21\n",
    "\n",
    "The OWER classifier does learn something - training loss decreases\n",
    " as does validation until the 5th epoch or so - but\n",
    "validation loss decreases only slightly, and the results on\n",
    "\"Barack Obama is a married, male, American actor\" look bad.\n",
    "\n",
    "It is worth turning the weights of the loss function but still the\n",
    "resulting classes for Obama differ randomly from run to run. Also,\n",
    "plotting the class/word attentions shows that those converge, but\n",
    "the observable correlations are not as expected.\n",
    "\n",
    "In the following a minimal example shall be constructed to proof\n",
    "the concept behind the classifier. An entity with a few short\n",
    "sentences from a small vocabulary shall be forwarded through a\n",
    "pre-trained OWER classifier whose class embeddings perfectly match\n",
    "the sentences so that the correct classes should be predicted.\n",
    "\n",
    "Example scenario:\n",
    "\n",
    "```\n",
    "Input sentences: \"married married\", \"male male\", \"american american\"\n",
    "Expected classes: married=True, male=True, american=True, actor=?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_tensor(_tensor, _title, _labels):\n",
    "    pass\n",
    "\n",
    "%run util.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Input\n",
    "\n",
    "Given an input such as `raw_input` in the following, pre-processing\n",
    "should yield a `sents_batch` and a `classes_batch`, each of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "raw_input = {\n",
    "    'ent': 123,\n",
    "    'classes': [1, 1, 1, 0],  # classes = married, male, american, actor\n",
    "    'sents': [\n",
    "        'married married married',\n",
    "        'male male male',\n",
    "        'American American American'\n",
    "    ]\n",
    "}\n",
    "\n",
    "import torch\n",
    "\n",
    "sents_batch = torch.tensor([[\n",
    "    [1, 1, 1],\n",
    "    [2, 2, 2],\n",
    "    [3, 3, 3]\n",
    "]])\n",
    "\n",
    "classes_batch = torch.tensor([[1, 1, 1, 0]])\n",
    "\n",
    "batch_size = 1\n",
    "sent_count = 3\n",
    "sent_len = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare EmbeddingBag\n",
    "\n",
    "Prepare an `EmbeddingBag` for 4 tokens:\n",
    "- 0 = unknown\n",
    "- 1 = 'married'\n",
    "- 2 = 'male'\n",
    "- 3 = 'American'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import EmbeddingBag\n",
    "from torch import tensor\n",
    "\n",
    "embedding_bag = EmbeddingBag(num_embeddings=4, embedding_dim=4)\n",
    "\n",
    "embedding_bag.weight.data = tensor([\n",
    "    [1., 0., 0., 0.],\n",
    "    [0., 1., 0., 0.],\n",
    "    [0., 0., 1., 0.],\n",
    "    [0., 0., 0., 1.]\n",
    "])\n",
    "\n",
    "print(embedding_bag)\n",
    "print(embedding_bag.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare class embeddings\n",
    "\n",
    "Prepare class embeddings for the 4 classes. Class embeddings 1-3 perfectly\n",
    "match tokens 1-3. Class embedding 0 doesn't match any token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_embs = tensor([\n",
    "    [0., 0., 0., 0.],\n",
    "    [0., 1., 0., 0.],\n",
    "    [0., 0., 1., 0.],\n",
    "    [0., 0., 0., 1.]\n",
    "])\n",
    "\n",
    "print(class_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare linear layer\n",
    "\n",
    "Prepare the final linear layer whose weights are set so that it\n",
    "completely relies on \"class 0 sentence mix\" for output class 0,\n",
    "\"class 1 sentence mix\" for output class 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "\n",
    "class_count = 4\n",
    "emb_size = 4\n",
    "\n",
    "linear = Linear(class_count * emb_size, class_count)\n",
    "\n",
    "linear.weight.data = tensor([\n",
    "    [1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.]\n",
    "])\n",
    "\n",
    "linear.bias.data = tensor([0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ent_labels = [f'ent {i}' for i in range(batch_size)]\n",
    "class_labels = ['married', 'male', 'American', 'actor']\n",
    "tok_labels = [f'tok {i}' for i in range(sent_len)]\n",
    "sent_labels = [f'sent {i}' for i in range(sent_count)]\n",
    "emb_labels = [f'emb {i}' for i in range(emb_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Forward\n",
    "\n",
    "#### 1.1 Embed Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Flatten batch\n",
    "#\n",
    "# < sents_batch     (batch_size, sent_count, sent_len)\n",
    "# > flat_sents      (batch_size * sent_count, sent_len)\n",
    "#\n",
    "\n",
    "flat_sents = sents_batch.reshape(batch_size * sent_count, sent_len)\n",
    "\n",
    "log_tensor(flat_sents, 'flat_sents',\n",
    "           [batch_size * sent_labels, tok_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Embed sentences\n",
    "#\n",
    "# < flat_sents      (batch_size * sent_count, sent_len)\n",
    "# > flat_sent_embs  (batch_size * sent_count, emb_size)\n",
    "#\n",
    "\n",
    "flat_sent_embs = embedding_bag(flat_sents)\n",
    "\n",
    "log_tensor(flat_sent_embs, 'flat_sent_embs',\n",
    "           [batch_size * sent_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Restore batch\n",
    "#\n",
    "# < flat_sent_embs      (batch_size * sent_count, emb_size)\n",
    "# > sent_embs_batch     (batch_size, sent_count, emb_size)\n",
    "#\n",
    "\n",
    "sent_embs_batch = flat_sent_embs.reshape(batch_size, sent_count, emb_size)\n",
    "\n",
    "log_tensor(sent_embs_batch, 'sent_embs_batch',\n",
    "           [ent_labels, sent_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.2 Calc attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Expand class embeddings for bmm()\n",
    "#\n",
    "# < class_embs          (class_count, emb_size)\n",
    "# > class_embs_batch    (batch_size, class_count, emb_size)\n",
    "#\n",
    "\n",
    "class_embs_batch = class_embs.expand(batch_size, class_count, emb_size)\n",
    "\n",
    "log_tensor(class_embs_batch, 'class_embs_batch',\n",
    "           [ent_labels, class_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Multiply each class with each sentence\n",
    "#\n",
    "# < class_embs_batch    (batch_size, class_count, emb_size)\n",
    "# < sent_embs_batch     (batch_size, sent_count, emb_size)\n",
    "# > atts_batch          (batch_size, class_count, sent_count)\n",
    "#\n",
    "\n",
    "atts_batch = torch.bmm(class_embs_batch, sent_embs_batch.transpose(1, 2))\n",
    "\n",
    "log_tensor(atts_batch, 'atts_batch',\n",
    "           [ent_labels, class_labels, sent_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Apply softmax over sentences\n",
    "#\n",
    "# < atts_batch      (batch_size, class_count, sent_count)\n",
    "# > softs_batch     (batch_size, class_count, sent_count)\n",
    "#\n",
    "\n",
    "from torch.nn import Softmax\n",
    "\n",
    "softs_batch = Softmax(dim=-1)(atts_batch)\n",
    "\n",
    "log_tensor(softs_batch, 'softs_batch',\n",
    "           [ent_labels, class_labels, sent_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Weight sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Repeat each batch slice class_count times\n",
    "#\n",
    "# < sent_embs_batch     (batch_size, sent_count, emb_size)\n",
    "# > expaned_batch       (batch_size, class_count, sent_count, emb_size)\n",
    "#\n",
    "\n",
    "expaned_batch = sent_embs_batch.unsqueeze(1).expand(-1, class_count, -1, -1)\n",
    "\n",
    "log_tensor(expaned_batch, 'expaned_batch',\n",
    "           [ent_labels, class_labels, sent_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Flatten sentences for bmm()\n",
    "#\n",
    "# < expaned_batch   (batch_size, class_count, sent_count, emb_size)\n",
    "# > flat_expanded   (batch_size * class_count, sent_count, emb_size)\n",
    "#\n",
    "\n",
    "flat_expanded = expaned_batch.reshape(-1, sent_count, emb_size)\n",
    "\n",
    "log_tensor(flat_expanded, 'flat_expanded',\n",
    "           [batch_size * class_labels, sent_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Flatten attentions for bmm()\n",
    "#\n",
    "# < softs_batch     (batch_size, class_count, sent_count)\n",
    "# > flat_softs      (batch_size * class_count, sent_count, 1)\n",
    "#\n",
    "\n",
    "flat_softs = softs_batch.reshape(batch_size * class_count, sent_count).unsqueeze(-1)\n",
    "\n",
    "log_tensor(flat_softs, 'flat_softs',\n",
    "           [batch_size * class_labels, sent_labels, ['']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Multiply each sentence with each attention\n",
    "#\n",
    "# < flat_expanded   (batch_size * class_count, sent_count, emb_size)\n",
    "# < flat_softs      (batch_size * class_count, sent_count, 1)\n",
    "# > flat_weighted   (batch_size * class_count, emb_size)\n",
    "#\n",
    "\n",
    "flat_weighted = torch.bmm(flat_expanded.transpose(1, 2), flat_softs).squeeze(-1)\n",
    "\n",
    "log_tensor(flat_weighted, 'flat_weighted',\n",
    "           [batch_size * class_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Restore batch\n",
    "#\n",
    "# < flat_weighted   (batch_size * class_count, emb_size)\n",
    "# > weighted_batch  (batch_size, class_count, emb_size)\n",
    "#\n",
    "\n",
    "weighted_batch = flat_weighted.reshape(batch_size, class_count, emb_size)\n",
    "\n",
    "log_tensor(weighted_batch, 'weighted_batch',\n",
    "           [ent_labels, class_labels, emb_labels])\n",
    "\n",
    "inputs_batch = weighted_batch.reshape(batch_size, class_count * emb_size)\n",
    "outputs_batch = linear(inputs_batch)\n",
    "\n",
    "log_tensor(outputs_batch, 'outputs_batch',\n",
    "           [ent_labels, class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "criterion = BCEWithLogitsLoss(pos_weight=torch.tensor([80] * class_count))\n",
    "\n",
    "loss = criterion(outputs_batch, classes_batch.float())\n",
    "loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}