{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feb 23\n",
    "\n",
    "Notebook 1, \"Simple Positive Case\" does not deliver very helpful results.\n",
    "It is not clear whether the prepared classifier weights represent the\n",
    "classifier state after being learned.\n",
    "\n",
    "A better test might be starting out with a classifier with random weights\n",
    "and trying to overfit on a single sample. The expected result would be\n",
    "learned weights that are similar to the preparation in notebook 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from IPython.lib.pretty import pretty\n",
    "from torch import Tensor\n",
    "from torch import tensor\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.nn import EmbeddingBag, Linear\n",
    "from torch.optim import Adam\n",
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def log_tensor(tensor_: Tensor, title: str, labels: List[List[str]]):\n",
    "    pass\n",
    "\n",
    "%run util.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def forward(embedding_bag: EmbeddingBag, class_embs: Tensor, linear: Linear, sents_batch: Tensor) -> Tensor:\n",
    "    pass\n",
    "\n",
    "%run classifier.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        'classes': [1, 1, 1],\n",
    "        'sents': [\n",
    "            'married married married',\n",
    "            'male male male',\n",
    "            'American American American'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'classes': [0, 0, 0],\n",
    "        'sents': [\n",
    "            'single single single',\n",
    "            'female female female',\n",
    "            'German German German'\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "batch_size = 2\n",
    "class_count = 3\n",
    "emb_size = 4\n",
    "sent_count = 3\n",
    "sent_len = 3\n",
    "\n",
    "class_labels = [f'class {i}' for i in range(class_count)]\n",
    "emb_labels = [f'emb {i}' for i in range(emb_size)]\n",
    "ent_class_labels = [f'ent {i} / class {j}' for i in range(batch_size) for j in range(class_count)]\n",
    "ent_labels = [f'ent {i}' for i in range(batch_size)]\n",
    "ent_sent_labels = [f'ent {i} / sent {j}' for i in range(batch_size) for j in range(sent_count)]\n",
    "mix_emb_labels = [f'mix {i} / class {j}' for i in range(class_count) for j in range(emb_size)]\n",
    "sent_labels = [f'sent {i}' for i in range(sent_count)]\n",
    "tok_labels = [f'tok {i}' for i in range(sent_len)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Pre-processing\n",
    "\n",
    "## 2.1 Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "words = [word for ent in data for sent in ent['sents'] for word in tokenize(sent)]\n",
    "vocab = Vocab(Counter(words))\n",
    "\n",
    "print(pretty(vocab.stoi))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "word_labels = [f'word {i}' for i in range(vocab_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Transform data\n",
    "\n",
    "Map words to tokens and create tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sents_batch = tensor([[[vocab[word] for word in tokenize(sent)] for sent in ent['sents']] for ent in data])\n",
    "classes_batch = torch.tensor([ent['classes'] for ent in data])\n",
    "assert len(sents_batch) == len(classes_batch)\n",
    "\n",
    "log_tensor(sents_batch, 'sents_batch', [ent_labels, sent_labels, tok_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3 Prepare classifier\n",
    "\n",
    "## 3.1 Create EmbeddingBag\n",
    "\n",
    "Create and prepare an `EmbeddingBag` with randomly distributed token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "embedding_bag = EmbeddingBag(num_embeddings=vocab_size, embedding_dim=emb_size)\n",
    "\n",
    "log_tensor(embedding_bag.weight.detach(), 'embedding_bag.weight', [word_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Create class embeddings\n",
    "\n",
    "Create randomly initialized class embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_embs = torch.rand((class_count, emb_size), requires_grad=True)\n",
    "init_class_embs = class_embs.detach().clone()\n",
    "\n",
    "log_tensor(class_embs.detach(), 'class_embs', [class_labels, emb_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Create linear layer\n",
    "\n",
    "Create a randomly initialized linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "linear = Linear(class_count * emb_size, class_count)\n",
    "\n",
    "log_tensor(linear.weight.data.detach(), 'linear.weight.data', [class_labels, mix_emb_labels])\n",
    "log_tensor(linear.bias.data.detach(), 'linear.bias.data', [class_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Forward & Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "criterion = BCEWithLogitsLoss(pos_weight=torch.tensor([80] * class_count))\n",
    "# criterion = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = Adam([embedding_bag.weight, class_embs, linear.weight, linear.bias], lr=0.1)\n",
    "\n",
    "epoch_count = 101\n",
    "for epoch in range(epoch_count):\n",
    "\n",
    "    if epoch in [0, 10, 100, 1000]:\n",
    "        print(epoch)\n",
    "\n",
    "        log_tensor(embedding_bag.weight.detach(), 'embedding_bag.weight', [word_labels, emb_labels])\n",
    "        log_tensor(class_embs.detach(), 'class_embs', [class_labels, emb_labels])\n",
    "        # log_tensor(linear.weight.data, 'linear.weight.data', [class_labels, mix_emb_labels])\n",
    "        # log_tensor(linear.bias.data, 'linear.bias.data', [class_labels])\n",
    "\n",
    "    logits_batch = forward(embedding_bag, class_embs, linear, sents_batch)\n",
    "\n",
    "    #\n",
    "    # Loss\n",
    "    #\n",
    "\n",
    "    loss = criterion(logits_batch, classes_batch.float())\n",
    "    print(loss.item())\n",
    "\n",
    "    # log_tensor(logits_batch, 'logits_batch', [ent_labels, class_labels], vmin=-1, vmax=1)\n",
    "    # log_tensor(classes_batch, 'classes_batch', [ent_labels, class_labels], vmin=-1, vmax=1)\n",
    "    # log_tensor(loss, 'loss', [])\n",
    "\n",
    "    #\n",
    "    # Backward\n",
    "    #\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5 Test\n",
    "\n",
    "## 5.1 Define test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        'classes': [1, 1, 1],\n",
    "        'sents': [\n",
    "            'married married married',\n",
    "            'male male male',\n",
    "            'American American American'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'classes': [0, 0, 0],\n",
    "        'sents': [\n",
    "            'single single single',\n",
    "            'female female female',\n",
    "            'German German German'\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "batch_size = 2\n",
    "class_count = 3\n",
    "emb_size = 4\n",
    "sent_count = 3\n",
    "sent_len = 3\n",
    "\n",
    "class_labels = [f'class {i}' for i in range(class_count)]\n",
    "emb_labels = [f'emb {i}' for i in range(emb_size)]\n",
    "ent_class_labels = [f'ent {i} / class {j}' for i in range(batch_size) for j in range(class_count)]\n",
    "ent_labels = [f'ent {i}' for i in range(batch_size)]\n",
    "ent_sent_labels = [f'ent {i} / sent {j}' for i in range(batch_size) for j in range(sent_count)]\n",
    "mix_emb_labels = [f'mix {i} / class {j}' for i in range(class_count) for j in range(emb_size)]\n",
    "sent_labels = [f'sent {i}' for i in range(sent_count)]\n",
    "tok_labels = [f'tok {i}' for i in range(sent_len)]\n",
    "\n",
    "# test_data = [\n",
    "#     {\n",
    "#         'classes': [1, 1, 1],  # married, male, American\n",
    "#         'sents': [\n",
    "#             'Barack Obama is married',\n",
    "#             'Barack Obama is male',\n",
    "#             'Barack Obama is American'\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         'classes': [1, 0, 1],  # married, male, American\n",
    "#         'sents': [\n",
    "#             'Michelle Obama is married',\n",
    "#             'Michelle Obama is female',\n",
    "#             'Michelle Obama is American'\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         'classes': [1, 0, 0],  # married, male, American\n",
    "#         'sents': [\n",
    "#             'Angela Merkel is married',\n",
    "#             'Angela Merkel is female',\n",
    "#             'Angela Merkel is German'\n",
    "#         ]\n",
    "#     }\n",
    "# ]\n",
    "#\n",
    "# batch_size = 3\n",
    "# # class_count = 3\n",
    "# # emb_size = 4\n",
    "# # sent_count = 3\n",
    "# sent_len = 4\n",
    "#\n",
    "# # class_labels = [f'class {i}' for i in range(class_count)]\n",
    "# # emb_labels = [f'emb {i}' for i in range(emb_size)]\n",
    "# ent_class_labels = [f'ent {i} / class {j}' for i in range(batch_size) for j in range(class_count)]\n",
    "# ent_labels = [f'ent {i}' for i in range(batch_size)]\n",
    "# ent_sent_labels = [f'ent {i} / sent {j}' for i in range(batch_size) for j in range(sent_count)]\n",
    "# # mix_emb_labels = [f'mix {i} / class {j}' for i in range(class_count) for j in range(emb_size)]\n",
    "# # sent_labels = [f'sent {i}' for i in range(sent_count)]\n",
    "# tok_labels = [f'tok {i}' for i in range(sent_len)]\n",
    "# # word_labels = [f'word {i}' for i in range(vocab_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Pre-process test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_sents_batch = tensor([[[vocab[word] for word in tokenize(sent)] for sent in ent['sents']] for ent in test_data])\n",
    "test_classes_batch = torch.tensor([ent['classes'] for ent in test_data])\n",
    "assert len(sents_batch) == len(classes_batch)\n",
    "\n",
    "log_tensor(test_sents_batch, 'test_sents_batch', [ent_labels, sent_labels, tok_labels])\n",
    "log_tensor(test_classes_batch, 'test_classes_batch', [ent_labels, class_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Forward test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_logits_batch = forward(embedding_bag, class_embs, linear, test_sents_batch)\n",
    "\n",
    "#\n",
    "# Loss\n",
    "#\n",
    "\n",
    "test_loss = criterion(test_logits_batch, test_classes_batch.float())\n",
    "\n",
    "log_tensor(test_logits_batch, 'test_logits_batch', [ent_labels, class_labels])\n",
    "log_tensor(test_classes_batch, 'test_classes_batch', [ent_labels, class_labels])\n",
    "log_tensor(test_loss, 'test_loss', [])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}