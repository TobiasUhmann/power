{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feb 28\n",
    "\n",
    "The loss curve indicates that training works, but it is not illustrative.\n",
    "Print some concrete examples in the training and validation loops to see\n",
    "how well training performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.display import display\n",
    "from torch import tensor, Tensor\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data import Field, TabularDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from classifier import Classifier\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ower_dataset = 'ower-v3-fb-irt-3'\n",
    "class_count = 4\n",
    "sent_count = 3\n",
    "\n",
    "# vectors = None\n",
    "# vectors = 'charngram.100d'\n",
    "# vectors = 'fasttext.en.300d'\n",
    "# vectors = 'fasttext.simple.300d'\n",
    "# vectors = 'glove.42B.300d'\n",
    "# vectors = 'glove.840B.300d'\n",
    "# vectors = 'glove.twitter.27B.25d'\n",
    "# vectors = 'glove.twitter.27B.50d'\n",
    "# vectors = 'glove.twitter.27B.100d'\n",
    "vectors = 'glove.twitter.27B.200d'\n",
    "# vectors = 'glove.6B.50d'\n",
    "# vectors = 'glove.6B.100d'\n",
    "# vectors = 'glove.6B.200d'\n",
    "# vectors = 'glove.6B.300d'\n",
    "\n",
    "emb_size = None\n",
    "# emb_size = 200\n",
    "\n",
    "batch_size = 1024\n",
    "sent_len = 64\n",
    "\n",
    "class_weight = 80\n",
    "lr = 0.01\n",
    "epoch_count = 20\n",
    "\n",
    "log_dir = 'runs/' + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \\\n",
    "          f'_{ower_dataset}' + f'_emb-{emb_size}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Build train/valid DataLoaders\n",
    "\n",
    "## 1.1 Read Sample TSVs into TabularDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Define columns for subsequent read into TabularDatasets\n",
    "#\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return text.split()\n",
    "\n",
    "raw_ent_field = Field(sequential=False, use_vocab=False)\n",
    "raw_class_field = Field(sequential=False, use_vocab=False)\n",
    "raw_sent_field = Field(sequential=True, use_vocab=True, tokenize=tokenize, lower=True)\n",
    "\n",
    "ent_field = ('ent', raw_ent_field)\n",
    "class_fields = [(f'class_{i}', raw_class_field) for i in range(class_count)]\n",
    "sent_fields = [(f'sent_{i}', raw_sent_field) for i in range(sent_count)]\n",
    "\n",
    "fields = [ent_field] + class_fields + sent_fields\n",
    "\n",
    "#\n",
    "# Read Train Samples TSV into TabularDataset\n",
    "#\n",
    "\n",
    "train_samples_tsv = f'data/{ower_dataset}/train.tsv'\n",
    "valid_samples_tsv = f'data/{ower_dataset}/valid.tsv'\n",
    "\n",
    "raw_train_set = TabularDataset(train_samples_tsv, 'tsv', fields, skip_header=True)\n",
    "raw_valid_set = TabularDataset(valid_samples_tsv, 'tsv', fields, skip_header=True)\n",
    "\n",
    "#\n",
    "# Print some samples\n",
    "#\n",
    "\n",
    "tab_cols = ['ent'] + [f'class_{i}' for i in range(class_count)] + [f'sent_{i}' for i in range(sent_count)]\n",
    "\n",
    "train_tab_data = [[getattr(row, col) for col in tab_cols] for row in raw_train_set[:20]]\n",
    "display(pd.DataFrame(train_tab_data, columns=tab_cols))\n",
    "\n",
    "valid_tab_data = [[getattr(row, col) for col in tab_cols] for row in raw_valid_set[:20]]\n",
    "display(pd.DataFrame(valid_tab_data, columns=tab_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2 Build vocab on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "raw_sent_field.build_vocab(raw_train_set, vectors=vectors)\n",
    "vocab = raw_sent_field.vocab\n",
    "\n",
    "#\n",
    "# Print some samples\n",
    "#\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab_size)\n",
    "print(vocab.itos[:10])\n",
    "print(vocab.itos[vocab_size//2:vocab_size//2+10])\n",
    "print(vocab.itos[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.3 Transfor each TabularDataset -> List[Sample]\n",
    "\n",
    "Parse texts from datasets, map words -> tokens (IDs) using vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sample:\n",
    "    ent: int\n",
    "    classes: List[int]\n",
    "    sents: List[List[int]]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter((self.ent, self.classes, self.sents))\n",
    "\n",
    "\n",
    "def transform(raw_set: TabularDataset) -> List[Sample]:\n",
    "    return [Sample(\n",
    "        int(getattr(row, 'ent')),\n",
    "        [int(getattr(row, f'class_{i}')) for i in range(class_count)],\n",
    "        [[vocab[token] for token in getattr(row, f'sent_{i}')] for i in range(sent_count)]\n",
    "    ) for row in raw_set]\n",
    "\n",
    "\n",
    "train_set = transform(raw_train_set)\n",
    "valid_set = transform(raw_valid_set)\n",
    "\n",
    "#\n",
    "# Print some samples\n",
    "#\n",
    "\n",
    "tab_cols = ['ent', 'classes', 'sents']\n",
    "tab_data = [[getattr(row, col) for col in tab_cols] for row in train_set[:3]]\n",
    "\n",
    "pd.DataFrame(tab_data, columns=tab_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.4 Build DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch: List[Sample]) -> Tuple[Tensor, Tensor]:\n",
    "\n",
    "    _ent, classes_batch, sents_batch = zip(*batch)\n",
    "\n",
    "    cropped_sents_batch = [[sent[:sent_len]\n",
    "                            for sent in sents] for sents in sents_batch]\n",
    "\n",
    "    padded_sents_batch = [[sent + [0] * (sent_len - len(sent))\n",
    "                           for sent in sents] for sents in cropped_sents_batch]\n",
    "\n",
    "    return tensor(padded_sents_batch), tensor(classes_batch)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=generate_batch, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2 Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if vocab.vectors is None:\n",
    "    classifier = Classifier.from_random(vocab, emb_size, class_count)\n",
    "else:\n",
    "    classifier = Classifier.from_pre_trained(vocab, class_count)\n",
    "\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# criterion = MSELoss()\n",
    "# criterion = BCEWithLogitsLoss()\n",
    "criterion = BCEWithLogitsLoss(pos_weight=torch.tensor([class_weight] * class_count))\n",
    "\n",
    "# optimizer = SGD(classifier.parameters(), lr=0.1)\n",
    "optimizer = Adam(classifier.parameters(), lr=lr)\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "\n",
    "    #\n",
    "    # Train\n",
    "    #\n",
    "    \n",
    "    train_samples = []\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for sents_batch, classes_batch in tqdm(train_loader):\n",
    "        logits_batch = classifier(sents_batch)\n",
    "\n",
    "        loss = criterion(logits_batch, classes_batch.float())\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #\n",
    "        # Append first sample of each batch to output\n",
    "        #\n",
    "\n",
    "        display_logits = [f'{x:.2f}' for x in logits_batch[0]]\n",
    "        display_classes = classes_batch[0].detach().numpy()\n",
    "        display_words = [[vocab.itos[ent] for ent in sent] for sent in sents_batch[0]]\n",
    "\n",
    "        train_samples.append([display_logits, display_classes] + display_words)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    #\n",
    "    # Validate\n",
    "    #\n",
    "    \n",
    "    valid_samples = []\n",
    "\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for sents_batch, classes_batch in tqdm(valid_loader):\n",
    "            logits_batch = classifier(sents_batch)\n",
    "\n",
    "            loss = criterion(logits_batch, classes_batch.float())\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            #\n",
    "            # Append first sample of each batch to output\n",
    "            #\n",
    "\n",
    "            display_logits = [f'{x:.2f}' for x in logits_batch[0]]\n",
    "            display_classes = classes_batch[0].detach().numpy()\n",
    "            display_words = [[vocab.itos[ent] for ent in sent] for sent in sents_batch[0]]\n",
    "\n",
    "            valid_samples.append([display_logits, display_classes] + display_words)\n",
    "\n",
    "    valid_loss /= len(valid_loader)\n",
    "    \n",
    "    train_df = pd.DataFrame(train_samples, columns=['pred', 'gt'] + [f'sent{i}' for i in range(sent_count)])\n",
    "    display(train_df)\n",
    "\n",
    "    valid_df = pd.DataFrame(valid_samples, columns=['pred', 'gt'] + [f'sent{i}' for i in range(sent_count)])\n",
    "    display(valid_df)\n",
    "\n",
    "    #\n",
    "    # Log\n",
    "    #\n",
    "\n",
    "    print(f'Epoch {epoch}: Train loss = {train_loss}, valid loss = {valid_loss}')\n",
    "    \n",
    "    writer.add_scalars('loss', {'train': train_loss, 'valid': valid_loss}, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4 Test\n",
    "\n",
    "## 4.1 Define test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        'ent': 1000,\n",
    "        'classes': [1, 0, 1, 0],  # married, male, American, actor\n",
    "        'sents': [\n",
    "            'Michelle is married',\n",
    "            'Michelle is female',\n",
    "            'Michelle is American'\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'ent': 2000,\n",
    "        'classes': [1, 0, 0, 0],  # married, male, American, actor\n",
    "        'sents': [\n",
    "            'Angela is married',\n",
    "            'Angela is female',\n",
    "            'Angela is German'\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "test_set = [Sample(\n",
    "    item['ent'],\n",
    "    item['classes'],\n",
    "    [[vocab[word] for word in tokenize(sent)] for sent in item['sents']]\n",
    ") for item in test_data]\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set), collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Forward test batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for sents_batch, classes_batch in test_loader:\n",
    "        logits_batch = classifier(sents_batch)\n",
    "        \n",
    "        print(logits_batch)\n",
    "\n",
    "        loss = criterion(logits_batch, classes_batch.float())\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "print(f'Test loss = {test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}