{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Feb 28\n",
    "\n",
    "It has turned out that the validation of the current\n",
    "ower-fb-3 dataset are not useful as all the ground truth\n",
    "classes are false for the first 300 entities.\n",
    "\n",
    "The script for building the OWER dataset needs to be\n",
    "examined and corrected. Estimated, the most frequent\n",
    "classes should be true at least for every 100th entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from dao.ower.ower_triples_db import DbTriple\n",
    "\n",
    "from dao.classes_tsv import ClassesTsv\n",
    "from dao.ower.ower_dir import OwerDir\n",
    "from dao.ryn.ryn_dir import RynDir\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ryn_dataset_dir = 'data/ryn/irt.cde.cde.1.clean'\n",
    "# ryn_dataset_dir = 'data/ryn/irt.cde.irt.1.clean'\n",
    "# ryn_dataset_dir = 'data/ryn/irt.cde.irt.5.clean'\n",
    "# ryn_dataset_dir = 'data/ryn/irt.cde.irt.15.clean'\n",
    "# ryn_dataset_dir = 'data/ryn/irt.cde.irt.30.clean'\n",
    "# ryn_dataset_dir = 'data/ryn/irt.fb.irt.1.clean'\n",
    "ryn_dataset_dir = 'data/ryn/irt.fb.irt.5.clean'\n",
    "# ryn_dataset_dir = 'data/ryn/irt.fb.irt.15.clean'\n",
    "# ryn_dataset_dir = 'data/ryn/irt.fb.irt.30.clean'\n",
    "# ryn_dataset_dir = 'data/ryn/irt.fb.owe.1.clean'\n",
    "\n",
    "classes_tsv = 'data/classes-v1-mmaa.tsv'\n",
    "\n",
    "sent_count = 3\n",
    "ower_dataset_dir = 'data/ower/ower-v3-fb-irt-3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1 Check files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Assert that (input) Ryn Directory exists\n",
    "#\n",
    "\n",
    "ryn_dir = RynDir('Ryn Directory', Path(ryn_dataset_dir))\n",
    "ryn_dir.check()\n",
    "\n",
    "#\n",
    "# Assert that (input) Classes TSV exists\n",
    "#\n",
    "\n",
    "classes_tsv = ClassesTsv('Classes TSV', Path(classes_tsv))\n",
    "classes_tsv.check()\n",
    "\n",
    "#\n",
    "# Create (output) OWER Dataset Directory if it does not exist already\n",
    "#\n",
    "\n",
    "ower_dir = OwerDir('OWER Directory', Path(ower_dataset_dir))\n",
    "ower_dir.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Create OWER dataset\n",
    "\n",
    "## 2.1 Load triples from Triples TXTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "split_dir = ryn_dir.split_dir\n",
    "cw_train_triples: List[Tuple[int, int, int]] = split_dir.cw_train_triples_txt.load_triples()\n",
    "cw_valid_triples: List[Tuple[int, int, int]] = split_dir.cw_valid_triples_txt.load_triples()\n",
    "ow_valid_triples: List[Tuple[int, int, int]] = split_dir.ow_valid_triples_txt.load_triples()\n",
    "ow_test_triples: List[Tuple[int, int, int]] = split_dir.ow_test_triples_txt.load_triples()\n",
    "\n",
    "train_triples = cw_train_triples + cw_valid_triples\n",
    "valid_triples = ow_valid_triples\n",
    "test_triples = ow_test_triples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.2 Save triples to Triples DBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ower_dir.train_triples_db.create_triples_table()\n",
    "train_db_triples = [DbTriple(triple[0], triple[1], triple[2]) for triple in train_triples]\n",
    "ower_dir.train_triples_db.insert_triples(train_db_triples)\n",
    "\n",
    "ower_dir.valid_triples_db.create_triples_table()\n",
    "valid_db_triples = [DbTriple(triple[0], triple[1], triple[2]) for triple in valid_triples]\n",
    "ower_dir.valid_triples_db.insert_triples(valid_db_triples)\n",
    "\n",
    "ower_dir.test_triples_db.create_triples_table()\n",
    "test_db_triples = [DbTriple(triple[0], triple[1], triple[2]) for triple in test_triples]\n",
    "ower_dir.test_triples_db.insert_triples(test_db_triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.3 Load entity sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_dir = ryn_dir.text_dir\n",
    "train_contexts: Dict[int, Set[str]] = text_dir.cw_train_sentences_txt.load_ent_to_sentences()\n",
    "valid_contexts: Dict[int, Set[str]] = text_dir.ow_valid_sentences_txt.load_ent_to_sentences()\n",
    "test_contexts: Dict[int, Set[str]] = text_dir.ow_test_sentences_txt.load_ent_to_sentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.4 Query each entity's classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classes: List[Tuple[int, int]] = classes_tsv.read_classes()\n",
    "\n",
    "train_class_to_entities = defaultdict(set)\n",
    "valid_class_to_entities = defaultdict(set)\n",
    "test_class_to_entities = defaultdict(set)\n",
    "\n",
    "for class_ in classes:\n",
    "    train_class_to_entities[class_] = ower_dir.train_triples_db.select_entities_with_class(class_)\n",
    "\n",
    "for class_ in classes:\n",
    "    valid_class_to_entities[class_] = ower_dir.valid_triples_db.select_entities_with_class(class_)\n",
    "\n",
    "for class_ in classes:\n",
    "    test_class_to_entities[class_] = ower_dir.test_triples_db.select_entities_with_class(class_)\n",
    "\n",
    "print()\n",
    "for k, v in train_class_to_entities.items():\n",
    "    print(k, len(v))\n",
    "\n",
    "print()\n",
    "for k, v in valid_class_to_entities.items():\n",
    "    print(k, len(v))\n",
    "    \n",
    "print()\n",
    "for k, v in test_class_to_entities.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.5 Save OWER TSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_tsv_rows = []\n",
    "valid_tsv_rows = []\n",
    "test_tsv_rows = []\n",
    "\n",
    "for ent in train_contexts:\n",
    "    train_tsv_row = [ent]\n",
    "    for class_ in classes:\n",
    "        train_tsv_row.append(int(ent in train_class_to_entities[class_]))\n",
    "    sentences = list(train_contexts[ent])[:sent_count]\n",
    "    if len(sentences) < sent_count:\n",
    "        continue\n",
    "    train_tsv_row.append(sentences)\n",
    "    train_tsv_rows.append(train_tsv_row)\n",
    "\n",
    "for ent in valid_contexts:\n",
    "    valid_tsv_row = [ent]\n",
    "    for class_ in classes:\n",
    "        valid_tsv_row.append(int(ent in valid_class_to_entities[class_]))\n",
    "    sentences = list(valid_contexts[ent])[:sent_count]\n",
    "    if len(sentences) < sent_count:\n",
    "        continue\n",
    "    valid_tsv_row.append(sentences)\n",
    "    valid_tsv_rows.append(valid_tsv_row)\n",
    "\n",
    "for ent in test_contexts:\n",
    "    test_tsv_row = [ent]\n",
    "    for class_ in classes:\n",
    "        test_tsv_row.append(int(ent in test_class_to_entities[class_]))\n",
    "    sentences = list(test_contexts[ent])[:sent_count]\n",
    "    if len(sentences) < sent_count:\n",
    "        continue\n",
    "    test_tsv_row.append(sentences)\n",
    "    test_tsv_rows.append(test_tsv_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ower_dir.train_samples_tsv.write_samples_tsv(train_tsv_rows)\n",
    "ower_dir.valid_samples_tsv.write_samples_tsv(valid_tsv_rows)\n",
    "ower_dir.test_samples_tsv.write_samples_tsv(test_tsv_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3 Check OWER TSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ent_to_label = ryn_dir.split_dir.entity_labels_txt.load_rid_to_label()\n",
    "converters = { 0: lambda ent: ent_to_label[int(ent)] }\n",
    "\n",
    "df = pd.read_csv(ower_dir.train_samples_tsv._path, sep='\\t', header=None, converters=converters)\n",
    "df.sample(frac=1)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(ower_dir.valid_samples_tsv._path, sep='\\t', header=None, converters=converters)\n",
    "df.sample(frac=1)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(ower_dir.test_samples_tsv._path, sep='\\t', header=None, converters=converters)\n",
    "df.sample(frac=1)[:100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}